Today, we'll discuss one of the most practical, widely used filtering method: Kalman Filter (KF). 

## 1. Filtering problem

Our world is highly complex. Understanding the "true" rule of the world, or the latent state of the world, is very hard but important problem to solve. There are many ideas and algorithms to approximately estimate the latent state of any system given a series of observations.

Bayesian inference is one of the powerful approach to solve this problem. We will not go into the details of Bayes' rule at this moment, but we can introduce a few problem definitions.

Let's assume the true system can be described as a hidden Markov model (HMM). For any true state $z_t$ at time $t$, we can observe a noisy observation $o_t$. Then, we can define the following:

$$
\begin{align*}

p(z_t|o_{1:T})&, \, t < T && \text{Smoothing problem} \\
p(z_t|o_{1:t})& && \text{Filtering problem} \\
p(z_{t+k}|o_{1:t})&, \, k > 0 && \text{Prediction / Planning problem} \\

\end{align*}
$$

Intuitively, we can say the filtering problem is "given the series of observations, including the current observation, what is the best estimate of the current state?"

<br>
<br>

## 2. Kalman Filter

### 2.1. Derivation

Kalman filter can be seen as a recursive Bayesian filter with a specific set of assumptions. First and foremost, Kalman filter assumes you have an access to the model - so, you already know the true system! This is indeed a strong assumption, and you may ask "If you already know the true system, why do you need a filter?" Well, sadly, Kalman filter does not assume you have a true access to the latent state $z_t$. So, estimating the latent state is still a problem worth solving.
The second assumption is that the true model is linear. This means that the model can be described as a linear function of the previous state and the current observation. The third assumption is that the noise (and posterior distribution) follows a Gaussian distribution.

With these assumptions, let's define the given model $\mathcal{M}$ with (potential) external control force $u_t$.

$$
\begin{align*}

z_t &= A_t z_{t-1} + B_t u_t + w_t, \quad w_t \sim \mathcal{N}(0, Q_t) & \text{State transition model} \\
o_t &= H_t z_t + v_t, \quad v_t \sim \mathcal{N}(0, R_t) & \text{Observation model} \\

\end{align*}
$$

Here we usually call $Q_t$ as a covariance of the process noise $w_t$, and $R_t$ as a covariance of the observation noise $v_t$. As you may have noticed, we do not strictly assume the noise to be stationary. However, again, we assume how they vary over time is completely known beforehand.

With this, we can take the first step of Kalman filtering: a prediction step.

$$
\begin{align*}

\hat{z}_{n \mid m} &\triangleq \mathbb{E}[p(z_n \mid o_{1:m})] & \text{Estimate mean} \\

\hat{z}_{t \mid {t-1}} &= \mathbb{E}[A_t\hat{z}_{t-1 \mid {t-1}} + B_t u_t + w_t] \\
&= A_t\hat{z}_{t-1 \mid {t-1}} + B_t u_t & \text{State prediction} \\
P_{t \mid {t-1}} &= \mathbb{E}[\text{cov}(z_t - \hat{z}_{t \mid t-1})] \\
&= \mathbb{E}[A_t(z_{t-1} - \hat{z}_{t-1 \mid {t-1}})(z_{t-1} - \hat{z}_{t-1 \mid {t-1}})^TA_t^T] + \mathbb{E}[w_t^2] \\
&= A_t P_{t-1 \mid {t-1}} A_t^T + Q_t & \text{Covariance prediction} \\

\end{align*}
$$

For this specific step, we have a filter estimation from the last step, $\hat{z}_{t-1 \mid t-1}$ and $P_{t-1 \mid t-1}$. As you can see, we leverage a fully known transition model to compute the "prior" estimate of the next state $\hat{z}_{t \mid t-1}$ and the covariance $P_{t \mid t-1}$ only with the observation up to $o_{1:t-1}$. In the section 2.2, we will make clear what we meant here by "prior". For now, our intuition is that we make a parametric estimation of the next state $z_t$ via tracking sufficient statistics of the multivariate Gaussian distribution, $\mathcal{N}(\mu, \Sigma) = \mathcal{N}(\hat{z}_{t \mid t-1}, P_{t \mid t-1})$.

The second step of Kalman filtering is an update step. This can be seen as incorporating a newly arrived observation $o_t$ to the prior estimate $\hat{z}_{t \mid t-1}$, yielding a posterior estimate $\hat{z}_{t \mid t}$. Here we leverage the observation model:

$$
\begin{align*}

\tilde{y}_t &= o_t - H_t \hat{z}_{t \mid t-1} & \text{Innovation, or residual} \\
S_t &= \mathbb{E}[\text{cov}(o_t - H_t \hat{z}_{t \mid t-1})] \\
&= \mathbb{E}[H_t(z_t - \hat{z}_{t \mid t-1})(z_t - \hat{z}_{t \mid t-1})^TH_t^T] + \mathbb{E}[v_t^2] \\
&= H_t P_{t \mid t-1} H_t^T + R_t & \text{Innovation covariance} \\

\end{align*}
$$

At this moment, let's restate our goal in a formal form. As a filtering problem, our goal is to minimize the prediction error $z_t - \hat{z}_{t \mid t}$. One straightforward approach is to solve a convex optimization problem via finding the minimum of the (element-wise) mean-squared error (MMSE):

$$
\begin{align*}

\mathbb{E}[(z_t - \hat{z}_{t \mid t})^2] &= \mathbb{E}[\text{tr}(\text{cov}(z_t - \hat{z}_{t \mid t}))] = \mathbb{E}[\text{tr}(P_{t \mid t})] \\

\end{align*}
$$

So, the problem boils down to minimize the trace of the covariance $P_{t \mid t}$. Though, how can we do that?

Well, notice that the new kids on the block when $o_t$ arrives are the innovation $\tilde{y}_t$ and the innovation covariance $S_t$. Let's see how we can leverage these two to update the prior estimate $\hat{z}_{t \mid t-1}$ to the posterior estimate $\hat{z}_{t \mid t}$.

Kalman filter takes a very simple idea, assuming a nice linear update function $K_t$:

$$
\begin{align*}

\hat{z}_{t \mid t} &= \hat{z}_{t \mid t-1} + K_t \tilde{y}_t & \text{Posterior state update} \\
P_{t \mid t} &= \mathbb{E}[\text{cov}(z_t - \hat{z}_{t \mid t})] \\
&= \mathbb{E}[\text{cov}(z_t - \hat{z}_{t \mid t-1} - K_t \tilde{y}_t)] \\
&= \mathbb{E}[\text{cov}(z_t - \hat{z}_{t \mid t-1} - K_t (o_t - H_t \hat{z}_{t \mid t-1}))] \\
&= \mathbb{E}[\text{cov}(z_t - \hat{z}_{t \mid t-1} - K_t (H_t z_t + v_t - H_t \hat{z}_{t \mid t-1}))] \\
&= \mathbb{E}[\text{cov}((I-K_tH_t)(z_t - \hat{z}_{t \mid t-1}) - K_t v_t)] \\
&= \mathbb{E}[\text{cov}((I-K_tH_t)(z_t - \hat{z}_{t \mid t-1}))] + \mathbb{E}[\text{cov}(K_t v_t)] \\
&= (I-K_tH_t) P_{t \mid t-1} (I-K_tH_t)^T + K_t R_t K_t^T & \text{Covariance update} \\

\end{align*}
$$

Then, voala! We know how to minimize the covariance $P_{t \mid t}$! We can simply take the derivative of the covariance update equation with respect to $K_t$. First, let's roll out the covariance update step:

$$
\begin{align*}

P_{t \mid t} &= (I-K_tH_t) P_{t \mid t-1} (I-K_tH_t)^T + K_t R_t K_t^T \\
&= P_{t \mid t-1} - K_t H_t P_{t \mid t-1} - P_{t \mid t-1} H_t^T K_t^T + K_t(H_t P_{t \mid t-1} H_t^T + R_t)K_t^T \\
&= P_{t \mid t-1} - K_t H_t P_{t \mid t-1} - P_{t \mid t-1} H_t^T K_t^T + K_t S_t K_t^T \\

\text{tr}(P_{t \mid t}) &= \text{tr}(P_{t \mid t-1}) - 2\text{tr}(P_{t \mid t-1} H_t^T K_t^T) + \text{tr}(K_t S_t K_t^T) & P \text{ is symmetric}\\

\frac{\partial \text{tr}(P_{t \mid t})}{\partial K_t} &= -2 P_{t \mid t-1} H_t + 2 K_t S_t & \text{Matrix derivative}\\

K_t &= P_{t \mid t-1} H_t^TS^{-1}_t & \text{Optimal Kalman gain} \\

\end{align*}
$$

Also, we can notice the optimal Kalman gain $K_t$ simplifies the covariance update term by canceling out $P_{t \mid t-1} H_t^TK_t^T$ and $K_tS_tK_t^T$. Therefore, given the exact value of the Kalman gain $K_t$, we can rewrite the update step as:

$$
\begin{align*}

\hat{z}_{t \mid t} &= (I-K_tH_t)\hat{z}_{t \mid t-1} + K_to_t & \text{Posterior state update} \\
P_{t \mid t} &= (I-K_tH_t)P_{t \mid t-1} & \text{Optimal covariance update} \\

\end{align*}
$$

Now we have a complete Kalman filter algorithm. Notice that the Kalman gain $K_t$ is independent of $o_t$: By recursively applying two steps for every new observation, we can estimate the latent state $z_t$ and the covariance $P_{t \mid t}$.

### 2.2. Kalman filter as a simple Bayesian filter
As we have a solid algorithm, let's concretely map each part of the equation to the Bayesian filter. First, with marginalization, the filtering problem can be written with Bayes' rule:

$$
p(z_t|o_{1:t}) = \frac{p(o_t|z_t)p(z_t|o_{1:t-1})}{p(o_t|o_{1:t-1})}
$$

The prediction step can be seen as a prior calculation, as mentioned previously. Before that, let me introduce a useful matrix algebra trick:

$$
\begin{align*}

\mathcal{N}(\mu_1, \Sigma_1) \mathcal{N}(\mu_2, \Sigma_2) &= \mathcal{N}(\Sigma_2(\Sigma_1+\Sigma_2)^{-1}\mu_1 + \Sigma_1(\Sigma_1+\Sigma_2)^{-1}\mu_2, \Sigma_1(\Sigma_1+\Sigma_2)^{-1}\Sigma_2) \\

\int \mathcal{N}(y \mid Az+b,C)\mathcal{N}(z \mid m,V)dz &= \mathcal{N}(y \mid Am+b, AVA^T + C) \\

\end{align*}
$$

This allows us to decompose the prior term with Kalman filter parameters.

$$
\begin{align*}
p(z_t|z_{t-1}) &= \mathcal{N}(A_tz_{t-1} + B_t u_t, Q_t) \\
p(z_{t-1}|o_{1:t-1}) &= \mathcal{N}(\hat{z}_{t-1 \mid t-1}, P_{t-1 \mid t-1}) \\
p(z_t|o_{1:t-1}) &= \int p(z_t|z_{t-1})p(z_{t-1}|o_{1:t-1})dz_{t-1} \\
&= \int \mathcal{N}(z_t|A_tz_{t-1} + B_t u_t, Q_t) \mathcal{N}(z_{t-1}|\hat{z}_{t-1 \mid t-1}, P_{t-1 \mid t-1}) dz_{t-1} \\
&= \mathcal{N}(z_t|A_t\hat{z}_{t-1 \mid t-1} + B_t u_t, A_t P_{t-1 \mid t-1} A_t^T + Q_t) \\
&= \mathcal{N}(z_t|\hat{z}_{t \mid t-1}, P_{t \mid t-1}) & \text{Prior calculation} \\

\end{align*}
$$

Likelihood calculation is thankfully straightforward, as an observation model itself:
$$
\begin{align*}
p(o_t|z_t) &= \mathcal{N}(H_t z_t, R_t) \\
\end{align*}
$$

Lastly, the evidence (or, marginal likelihood) term requires a product of multivariate Gaussian distributions.

$$
\begin{align*}

p(o_t|o_{1:t-1}) &= \int p(o_t|z_t)p(z_t|o_{1:t-1})dz_t \\
&= \int \mathcal{N}(o_t \mid H_t z_t, R_t) \mathcal{N}(z_t|\hat{z}_{t \mid t-1}, P_{t \mid t-1}) dz_t \\
&= \mathcal{N}(o_t \mid H_t\hat{z}_{t \mid t-1}, H_t P_{t \mid t-1} H_t^T + R_t) \\
&= \mathcal{N}(o_t \mid H_t\hat{z}_{t \mid t-1}, S_t) & \text{Evidence calculation} \\

\end{align*}
$$

The evidence term actually gives as a nice interpretation of the Kalman gain $K_t$. Recall the optimal Kalman gain $K_t$ is defined as:
$$
\begin{align*}
K_t &= P_{t \mid t-1} H_t^TS^{-1}_t \\
&= \frac{P_{t \mid t-1} H_t^T}{H_t P_{t \mid t-1} H_t^T + R_t} \\

\end{align*}
$$
This means that the Kalman gain $K_t$ is correlated with the precision of the evidence, $S_t$. The more precise the evidence is, the more we trust the observation.

The intuitive interpretation is the signal-to-noise ratio (SNR). If SNR for the observation at time $t$ is high, then $R_t$ is expected to be small. Then, the Kalman gain $K_t$ would approximately be an identity matrix, meaning we trust the observation more than the prior estimate. On the other hand, if SNR is low, then $R_t$ is expected to be large. Then, the Kalman gain $K_t$ would approximately be a zero matrix, meaning we trust the prior estimate more than the observation.