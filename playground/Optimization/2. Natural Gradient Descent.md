Last time, we discussed the Newton's method, which is a second-order optimization algorithm. It uses the Hessian matrix to find the optimal point. However, there is one problem we should consider: both gradient descent and standard Newton's method are not invariant to the parameterization of the model. This leads to the search for a more robust and "natural" optimization method.

# 1. Bregman Divergence
For the beginning, think about the gradient descent method. Let me define a simple linear approximation of the function $f$ at the point $x$:
$$
f(x + \Delta x) \approx f(x) + \nabla f(x)^T \Delta x
$$
where $\Delta x$ is a small perturbation. This is straightforward: we find the direction of the steepest descent and move a small step in that direction. Notice that we can throw some regularization term in to provide a more robust solution, especially for the stochastic case:
$$
x_{t+1} = \operatorname{argmin}_{x} \left( f(x_t) + \nabla f(x_t)^T (x - x_t) + \frac{\lambda}{2} \|x - x_t\|_2^2 \right)
$$
where $\lambda$ is a regularization parameter - or, inverse learning rate in gradient descent. Taking the derivative with respect to $x$, we get:
$$
\begin{align*}
\nabla f(x_t) + \lambda (x - x_t) &= 0 \\
x_{t+1} &= x_t - \frac{1}{\lambda} \nabla f(x_t)
\end{align*}
$$
which brings us back to the standard gradient descent. 

One generalization idea here is to use a different metric for the regularization, so that we can replace $\|x - x_t\|_2^2$ with more general $D(x, x_t)$. One of the general choices is Bregman divergence, which is defined as:
$$
D_\phi(y, x) = \phi(y) - (\phi(x) + \langle \nabla \phi(x), (y - x) \rangle)
$$
where $\phi$ is a convex function. Intuitively, Bregman divergence measures the first-order approximation error of the function $\phi$ at the point $x$ with respect to the point $y$. From this lense, we can again see the role of the regularization term in gradient descent: we don't trust our first-order approximation to be hold globally.

Bregman divergence casually recovers many metrics and divergences we use. Simply, if we take $\phi(x) = \frac{1}{2} \|x\|_2^2$, we recover the squared Euclidean distance:
$$
\begin{align*}
D_\phi(x+\Delta x, x) &= \frac{1}{2} \|x+\Delta x\|_2^2 - \frac{1}{2} \|x\|_2^2 - \langle x, \Delta x \rangle \\
&= \frac{1}{2} (\|x\|_2^2 + \|\Delta x\|_2^2 + 2 \langle x, \Delta x \rangle) - \frac{1}{2} \|x\|_2^2 - \langle x, \Delta x \rangle \\
&= \frac{1}{2} \|\Delta x\|_2^2 \\
&= \frac{1}{2} \|(x+\Delta x) - x\|_2^2
\end{align*}
$$

Generalizing $\phi(x)$ to be the quadratic function $\phi(x) = \frac{1}{2} x^T A x$ for some $A \succeq 0$, we get:
$$
\begin{align*}
D_\phi(x+\Delta x, x) &= \frac{1}{2} (x+\Delta x)^T A (x+\Delta x) - \frac{1}{2} x^T A x - \langle Ax, \Delta x \rangle \\
&= \frac{1}{2} (x^T A x + \Delta x^T A \Delta x + 2 \langle Ax, \Delta x \rangle) - \frac{1}{2} x^T A x - \langle Ax, \Delta x \rangle \\
&= \frac{1}{2} \Delta x^T A \Delta x + \langle Ax, \Delta x \rangle - \langle Ax, \Delta x \rangle \\
&= \frac{1}{2} \Delta x^T A \Delta x
\end{align*}
$$
which is the squared Mahalanobis distance.

Lastly, with the choice of unnormalized negative entropy function $\phi(x) = \sum_i x_i \log x_i - x_i$ where $\sum_i x_i = 1$, we get the KL divergence:
$$
\begin{align*}
D_\phi(x+\Delta x, x) &= \sum_i \left[ (x_i + \Delta x_i) \log (x_i + \Delta x_i) - (x_i + \Delta x_i) \right] - \sum_i \left[x_i \log x_i - x_i \right] - \langle \log x_i, \Delta x \rangle \\
&= \sum_i (x_i + \Delta x_i) \frac{x_i + \Delta x_i}{x_i} - \sum_i \Delta x_i \\
&= D_{KL}(x+\Delta x \| x) - \sum_i ((x_i + \Delta x_i) - x_i) \\
&= D_{KL}(x+\Delta x \| x) &\text{Since x is a probability distribution}
\end{align*}
$$

With this, we can generalize the gradient descent update rule to any Bregman divergence space:
$$
\begin{align*}
x_{t+1} &= \operatorname{argmin}_{x} \left( f(x_t) + \nabla f(x_t)^T (x - x_t) + D_\phi(x, x_t) \right) \\
0 &= \nabla f(x_t) + \nabla \phi(x) - \nabla \phi(x_t) &\text{Set gradient to }0 \\
x_{t+1} &= \nabla \phi^{-1} \left( \nabla \phi(x_t) - \nabla f(x_t) \right) &\text{Solve for }x
\end{align*}
$$
where $\nabla \phi^{-1}$ is the inverse of the gradient of $\phi$. This is a very general form of the update rule, and we can use it to derive many different optimization algorithms. Let's check with simple examples:
$$
\begin{align*}
\phi(x) &= \frac{1}{2} \|x\|_2^2 \\
\nabla \phi(x) &= x \\
\nabla \phi^{-1}(y) &= y \\
x_{t+1} &= \nabla \phi^{-1} \left( x_t - \nabla f(x_t) \right) = x_t - \nabla f(x_t) &\text{Standard gradient descent}
\end{align*}
$$

<br>
<br>

# 2. Exponential Family

Before we move to the natural gradient descent, let's check the exponential family of distributions. The exponential family is a class of probability distributions that can be expressed in the following form:
$$
p(x \mid \theta) = h(x) \exp\left[\langle \eta(\theta), T(x) \rangle - A(\theta)\right]
$$
Well, this looks monstrous - so let me tenderize it a bit. 

- $\eta(\theta)$ is the "natural," or "canonical" parameter of the distribution. We say $\theta$ is natural when $\eta(\theta) = \theta$. For natural parameters we will just omit details and write $\eta$.
- $T(x)$ is the "sufficient statistics" of the distribution, estimated from data. Mean parameter $\mu$ is defined as $\mathbb{E}[T(x)]$.
- $A(\theta)$ is the "log-partition function" or "cumulant function." It is a normalizing constant that ensures the distribution integrates to 1.
- $h(x)$ is the "base measure" of the distribution. It is a function of $x$ that does not depend on $\theta$.
- Importantly, we call $p(x \mid \theta)$ to be minimal (or, regular) if $T(x)$ is linearly independent. This means there is no redundant information in the sufficient statistics.

Well, that didn't help much I guess! Let's check some details.

First, let's integrate both sides of the equation with natural parameters. Here we use Lebesgue integral with a specific measure $v$,
$$
\begin{align*}
\int p(x \mid \eta) v(dx) &= \int h(x) \exp(\langle \eta, T(x) \rangle)\exp(-A(\eta)) v(dx) = 1 \\
\exp(A(\eta)) &= \int h(x) \exp(\langle \eta, T(x) \rangle) v(dx) \\
A(\eta) &= \log \int h(x) \exp(\langle \eta, T(x) \rangle) v(dx)
\end{align*}
$$
In that, $A(\eta)$ is introduced only to ensure the distribution integrates to 1. If you are not familiar with Lebesgue integral, it is fine to just think about standard Riemannian integral and ignore $v$. We introduced Lebesgue integral only to generalize integration over some distribution whose measure does not span continuous space, like Bernoulli distribution where $v = \{0,1\}$ counting measure.

The beauty of the exponential family is that, as we choose $T(x)$, $h(x)$, and $\eta$, we can recover many familiar distributions. Before we lose ourselves in the details, let's check some examples.

### 2.1. Some famous members of the exponential family
#### 2.1.1. Bernoulli distribution

$$
\begin{align*}
p(x \mid p) &= p^x (1-p)^{1-x} && \text{Bernoulli distribution} \\
&= \exp\left[x \log p + (1-x) \log(1-p)\right] \\
&= 1 \cdot \exp\left[x \log \frac{p}{1-p} + \log(1-p)\right] \\
&= h(x) \exp\left[\langle \eta, T(x) \rangle - A(\eta)\right] \\
&= \exp\left[\eta x - \log(1+\exp(\eta))\right] && \because \log (1+\exp(\eta)) = A(\eta) \\
&= \sigma(-\eta)\exp(\eta x) \\
&= p(x \mid \eta) && \text{Natural parameterization}
\end{align*}
$$

So, parameterization of Bernoulli distribution is:
- $v = \{0,1\}$ counting measure
- $\eta = \log \frac{p}{1-p}$
- $T(x) = x$
- $h(x) = 1$
- Then, normalizing $A(\eta) = -\log(1 - p) = \log(1 + e^\eta)$

#### 2.1.2. Gaussian distribution
Beware: here I abused and overloaded notation. Outside this section, $\mu$ is a mean parameter for general exponential family, but here it is a mean of the Gaussian distribution. Also, in the previous section I used $\sigma$ for the sigmoid function, but here it is a standard deviation of the Gaussian distribution.
$$
\begin{align*}
p(x \mid \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right] && \text{Gaussian distribution} \\
&= \frac{1}{\sqrt{2\pi}} \cdot \exp\left[\frac{\mu x}{\sigma^2} - \frac{x^2}{2\sigma^2} - \frac{\mu^2}{2\sigma^2} - \log \sigma \right] \\
&= h(x) \exp\left[\langle \eta, T(x) \rangle - A(\eta)\right] \\
\end{align*}
$$
where
- $v = \mathbb{R}$ Lebesgue measure
- $\eta = \left[\frac{\mu}{\sigma^2}, -\frac{1}{2\sigma^2}\right]$
- $T(x) = \left[x, x^2\right]$
- $h(x) = \frac{1}{\sqrt{2\pi}}$
- Therefore, $A(\eta) = \frac{\mu^2}{2\sigma^2} + \log \sigma = -\frac{\eta_1^2}{4\eta_2} - \frac{1}{2}\log(-2\eta_2)$.

### 2.2. $A(\eta)$ as a moment generating function
Exponential family has a very nice property: the moment generating function of the distribution is given by the log-partition function $A(\eta)$. For today, we will only focus on the first and second moments. By taking the derivative of $A(\eta)$ with respect to $\eta$:
$$
\begin{align*}
\frac{\partial A(\eta)}{\partial \eta} &= \frac{\partial}{\partial \eta} \log \int h(x) \exp(\langle \eta, T(x) \rangle) v(dx) \\
&= \frac{\int T(x) h(x) \exp(\eta^T T(x)) v(dx)}{\int h(x) \exp(\eta^T T(x)) v(dx)} && \text{Dominated convergence theorem} \\
&= \frac{\int T(x) h(x) \exp(\eta^T T(x)) v(dx)}{\exp(A(\eta))} \\
&= \int T(x) h(x) \exp(\eta^T T(x) - A(\eta)) v(dx) \\
&= \mathbb{E}_{p(x \mid \eta)}[T(x)] && \text{Expected sufficient statistics} \\
&= \mu(\eta) && \text{Mean parameter}
\end{align*}
$$

Similarly, we can take the second derivative of $A(\eta)$:
$$
\begin{align*}
\frac{\partial^2 A(\eta)}{\partial \eta^2} &= \frac{\partial}{\partial \eta} \mathbb{E}_{p(x \mid \eta)}[T(x)] \\
&= \int T(x) h(x) (T(x) - \frac{\partial}{\partial \eta} A(\eta))^T \exp(\eta^T T(x) - A(\eta)) v(dx) \\
&= \int T(x) h(x) (T(x) - \mathbb{E}_{p(x \mid \eta)}[T(x)])^T \exp(\eta^T T(x) - A(\eta)) v(dx) \\
&= \int T(x) h(x) T(x)^T \exp(\eta^T T(x) - A(\eta)) v(dx) - \int T(x) h(x) \mathbb{E}_{p(x \mid \eta)}[T(x)]^T \exp(\eta^T T(x) - A(\eta)) v(dx) \\
&= \mathbb{E}_{p(x \mid \eta)}[T(x) T(x)^T] - \mathbb{E}_{p(x \mid \eta)}[T(x)] \mathbb{E}_{p(x \mid \eta)}[T(x)]^T \\
&= \text{Var}_{p(x \mid \eta)}[T(x)] && \text{Variance of sufficient statistics}
\end{align*}
$$

### 2.3. Relation between $\eta$ and $\mu$
From the last section, we have a nice property $\mu = \frac{\partial A}{\partial \eta}$. Furthermore, we will see there is indeed a one-to-one inverse mapping between $\eta$ and $\mu$ for minimal exponential family.

First, let's check the curvature of $A(\eta)$:
$$
\begin{align*}
\eta &= \lambda \eta_1 + (1 - \lambda) \eta_2 \\
\exp(A(\eta)) &= \int h(x) \exp [\eta^T T(x)] v(dx) \\
&= \int h(x) \exp [\lambda \eta_1^T T(x) + (1 - \lambda) \eta_2^T T(x)] v(dx) \\
&= \int \{h(x)\exp \eta_1^T T(x)\}^\lambda \{h_x\exp \eta_2^T T(x)\}^{1-\lambda} v(dx) \\
&\le \left( \int h(x)\exp \eta_1^T T(x)\ v(dx) \right)^\lambda \left( \int h(x)\exp \eta_2^T T(x) v(dx) \right)^{1-\lambda} && \text{H\"older's inequality} \\
&= \exp(\lambda A(\eta_1) + (1 - \lambda) A(\eta_2)) \\
A(\lambda \eta_1 + (1 - \lambda) \eta_2) &\le \lambda A(\eta_1) + (1 - \lambda) A(\eta_2) && A(\eta)\text{ is convex} \\
\end{align*}
$$
$\text{H\"older's}$ inequality is strict unless $\exp(\eta_1^T T(x))$ and $\exp(\eta_2^T T(x))$ are linearly dependent. Therefore, with minimal exponential family, $A(\eta)$ is strictly convex. This implies that the relation $\mu = \frac{\partial A}{\partial \eta}$ is invertible, and we can write $\eta = \Phi(\mu)$ for some function $\Phi$.

This is a very important property of the exponential family. Let's check the Fisher information matrix of the distribution w.r.t. $\eta$:
$$
\begin{align*}
F_\eta &= -\mathbb{E}_{p(x \mid \eta)}\left[\frac{\partial^2 \log p(x \mid \eta)}{\partial \eta^2}\right] && \text{Variance of the score function} \\
&= -\mathbb{E}_{p(x \mid \eta)}\left[\frac{\partial}{\partial \eta} \frac{\partial \log p(x \mid \eta)}{\partial \eta}\right] \\
&= -\mathbb{E}_{p(x \mid \eta)}\left[\frac{\partial}{\partial \eta} \frac{\partial (\log h(x) + \eta^TT(x) - A(\eta))}{\partial \eta}\right] \\
&= -\mathbb{E}_{p(x \mid \eta)}\left[\frac{\partial}{\partial \eta} \left(T(x) - \frac{\partial A(\eta)}{\partial \eta}\right)\right] \\
&= \mathbb{E}_{p(x \mid \eta)}\left[\frac{\partial}{\partial \eta} \mu \right] \\
&= \frac{\partial \mu}{\partial \eta} \\
&= \text{Var}_{p(x \mid \eta)}[T(x)] && \text{Variance of sufficient statistics} \\
\end{align*}
$$

With the change of variables $\eta = \Phi(\mu)$, we can write the Fisher information matrix in terms of $\mu$:
$$
\begin{align*}
F_\eta &= \mathbb{E}_{p(x \mid \eta)}\left[\frac{\partial \log p(x \mid \eta)}{\partial \eta} \frac{\partial \log p(x \mid \eta)}{\partial \eta}\right] \\
&= \mathbb{E}_{p(x \mid \eta)}\left[\left(\frac{\partial \log p(x \mid \eta)}{\partial \mu}{\frac{\partial \mu}{\partial \eta}}\right)^2\right] \\
&= F_\eta^T F_\mu F_\eta \\
F_\eta &= F_\mu^{-1} && \text{Inverse Relation} \\
\end{align*}
$$

We will leverage this inverse relation later.

<br>
<br>

# 3. Natural Gradient Descent
It was a rather dense journey, but we are finally here. The natural gradient descent is a generalization of the standard gradient descent, where we use KL divergence as a Bregman divergence regularization metric (although KL is not strictly a metric).

Our goal is to regularize the gradient descent update rule, but now in the space of probability distributions. To make our previous sections worth, we assume the distribution we optimize belongs to the exponential family. Let's write it down, considering the optimization problem w.r.t. the natural parameter $\eta$:
$$
\begin{align*}
\eta_{t+1} &= \operatorname{argmin}_{\eta} \left( f(\eta_t) + \nabla f(\eta_t)^T (\eta - \eta_t) + D_{KL}(p(x \mid \eta) \| p(x \mid \eta_t)) \right) \\
&\approx \operatorname{argmin}_{\eta} \left( f(\eta_t) + \nabla f(\eta_t)^T (\eta - \eta_t) + \frac{1}{2}(\eta - \eta_t)^T F_\eta (\eta - \eta_t) \right) \\
\nabla f(\eta_t) + F_\eta (\eta - \eta_t) &= 0 \\
\eta_{t+1} &= \eta_t - F_\eta^{-1} \nabla f(\eta_t) \\
\end{align*}
$$

With the same derivation, we can write the update rule w.r.t. the mean parameter $\mu$. This leads to an interesting relation:
$$
\begin{align*}
\mu_{t+1} &= \mu_t - F_\mu^{-1} \nabla_\mu f(\mu_t) && \text{Natural Gradient Descent w.r.t. } \mu \\
&= \mu_t - F_\eta \nabla_\mu \eta \nabla_\eta f(\mu_t) && \text{Inverse relation} \\
&= \mu_t - F_\eta F_\eta^{-1} \nabla_\eta f(\mu_t) \\
&= \mu_t - \nabla_\eta f(\mu_t) && \text{Standard Gradient Descent}
\end{align*}
$$

This is a very interesting result. The natural gradient vector w.r.t. one of the coordinate, either $\eta$ or $\mu$, corresponds to the standard gradient vector w.r.t. the other coordinate. This means that, if we have a natural parameterization of the distribution, we can use the standard gradient descent to optimize the mean parameter. This is a very powerful result, as it frees us from the need to compute the inverse of Fisher information matrix.

### 3.1. Natural Gradient is invariant to parameterization