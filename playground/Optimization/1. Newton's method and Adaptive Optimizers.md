*20250311 This section deserves much better explanation.* 
# 1. What is Newton's method?

Let's say we have a scalar loss function $\mathcal{L}(\theta): \mathbb{R^N} \to \mathbb{R}$, and we want to find the minimum of this function. One, arguably the most straightforward, way to minimize the loss function given some parameter $\theta$ is to use the gradient descent algorithm. The gradient descent algorithm updates the parameter $\theta$ in the opposite direction of the gradient of the loss function.

Compared to the gradient descent which is a first-order optimization algorithm, Newton's method is a second-order optimization algorithm that uses the second derivative of the loss function on top of the gradient information.

Why would the Hessian, the second derivative of the loss function, be useful in optimization? One intuition (crude and might be wrong) is that any (local) minima would look like a parabola. By fitting a parabola i.e. a quadratic function to the loss function, we can estimate not only the direction but also the position of the minimum.

Not that surprisingly, we would love to do some second-order Taylor expansion around the current parameter $\theta$:

$$
\mathcal{L}(\theta + \Delta \theta) \approx \mathcal{L}(\theta) + \nabla \mathcal{L}(\theta)^T \Delta \theta + \frac{1}{2} \Delta \theta^T \nabla^2 \mathcal{L}(\theta) \Delta \theta
$$

where $\nabla \mathcal{L}(\theta) \in \mathbb{R^N \times 1}$ is the gradient of the loss function and $\nabla^2 \mathcal{L}(\theta) \in \mathbb{R^{N \times N}}$ is the Hessian of the loss function. Note that every term in the above equation should be a scalar - this helps with the intuition for the transpose and multiplication operations.

Then, our goal is to find the $\Delta \theta$ that minimizes the loss. We can do this by taking the derivative of the above equation with respect to $\Delta \theta$ and setting it to zero:

$$
\begin{align*}

\frac{\partial \mathcal{L}(\theta + \Delta \theta)}{\partial \Delta \theta} &\approx \nabla \mathcal{L}(\theta) + \nabla^2 \mathcal{L}(\theta)\Delta \theta && \text{Setting the derivative to zero} \\

\Delta \theta &= -\left[\nabla^2 \mathcal{L}(\theta) \right]^{\dagger} \nabla \mathcal{L}(\theta) && \text{Moore-Penrose pseudoinverse} \\

\theta_{t+1} &= \theta_t - \left[\nabla^2 \mathcal{L}(\theta) \right]^{\dagger} \nabla \mathcal{L}(\theta) && \text{Update rule}

\end{align*}
$$

We iterate this update rule until the loss function converges. The update rule is the same as the gradient descent algorithm but also involves the Hessian of the loss function. Knowing the local curvature, you can see we did not need to specify the learning rate $\eta$ as in the gradient descent algorithm (although we surely can introduce an adaptive step size, or learning rate, to the Newton's method too!).

<br>
<br>

# 2. Gauss-Newton method
Theoretically, Newton's method is very powerful, even compared to the gradient descent algorithm. However, we have many practical issues when applying Newton's method to neural network optimization. For this section, notable issues are:

- The Hessian matrix is computationally expensive to compute and store, let alone its inverse.
- The Hessian matrix might be singular or ill-conditioned.

Gauss-Newton method is a lovely way to address the first issue.

One way to improve the Hessian estimation is to approximate the Hessian matrix with the positive semidefinite matrix $M = D^TD$ for some matrix $D$. Then, what would be a good choice for the matrix $D$ that approximates the Hessian matrix well? Let's assume our loss function is a quadratic loss function:

$$
\begin{align*}

\mathcal{L}(\theta) &= |f(x, \theta) - y|^2 && \text{for arbitrary $f(x, \theta)$} \\

\nabla \mathcal{L}(\theta) &= 2(f(x, \theta) - y) \nabla f(x, \theta) && \text{Gradient of the loss function} \\

\nabla^2 \mathcal{L}(\theta) &= \nabla \left(2(f(x, \theta) - y) \nabla f(x, \theta)\right) \\
&= 2 \nabla f(x, \theta) \nabla f(x, \theta)^T + 2(f(x, \theta) - y) \nabla^2 f(x, \theta) && \text{Hessian of the loss function} \\
&\approx 2 \nabla f(x, \theta) \nabla f(x, \theta)^T && \text{Set } D = \nabla f(x, \theta) \\

&= \frac{2 \nabla \mathcal{L}(\theta) \nabla \mathcal{L}(\theta)^T}{4 (f(x, \theta) - y)} \\

&= \frac{\nabla \mathcal{L}(\theta) \nabla \mathcal{L}(\theta)^T}{2\mathcal{L}(\theta)} && \text{Gauss-Newton approximation}

\end{align*}
$$

You can commonly find notations like $\nabla^2 \mathcal{L}(\theta) = H \approx J^TJ$ where $H \in \mathbb{R^{N \times N}}$ is the Hessian matrix and $\nabla \mathcal{L}(\theta)^T = J \in \mathbb{R^{1 \times N}}$ is the Jacobian matrix. As all we need is the square of the Jacobian matrix, we can mitigate the computational cost of the Hessian matrix. Using this notation, we can rewrite the update rule as:
$$
\begin{align*}

\Delta \theta &= -\left[\nabla^2 \mathcal{L}(\theta) \right]^{\dagger} \nabla \mathcal{L}(\theta) && \text{Moore-Penrose pseudoinverse} \\

&= -\frac{J^T}{H} && \text{Different notation} \\
&\approx -\frac{J^T}{J^TJ} && \text{Gauss-Newton approximation} \\
&= -\frac{1}{J} && \text{Inverse of the Jacobian matrix}\\

\end{align*}
$$

Sadly, the square of the Jacobian matrix $J^TJ$ is not guaranteed to be positive definite (it is positive semidefinite). Therefore, it is still possible to encounter the ill-conditioned matrix problem. To address this issue, we might be able to add a small positive value to the diagonal of the matrix to make it positive definite (like, $J^TJ + \lambda I$)

<details>
    <summary> Generalization to the higher dimension </summary>

blah blah...

</details>

<br>
<br>

# 3. Adaptive optimizers

What we learned so far is that the Newton's method is powerful but computationally expensive. The Gauss-Newton method is a good approximation to the Newton's method. Turns out, many commonly used optimization algorithms such as Adagrad, RMSProp, or everyone's favorite Adam are easy to understand from the perspective of the Newton's method!

Let's look into these step by step.

### 3.1. Adagrad
Let's see the pseudocode of the Adagrad algorithm:

```python
c_grad_square = 0

while True:
    ## Get the gradient of the loss function
    gradient = compute_gradient()

    ## Cumulant here is the element-wise square of the gradient
    c_grad_square += gradient ** 2

    ## Update the parameter
    theta = theta - (learning_rate * gradient) / (np.sqrt(c_grad_square) + epsilon)
```

As this simple pseudocode shows, the update algorithm of the Adagrad follows:
$$
\begin{align*}
g_t &= \nabla \mathcal{L}(\theta_t) && \text{Gradient of the loss function} \\
v_t &= v_{t-1} + g_t^2 && \text{Cumulant of the gradient} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t && \text{Update rule}
\end{align*}
$$

But why did Adagrad put the square root of the cumulant in the update rule? Turns out, the element-wise square of the gradient is a diagonal elements of the Gauss-Newton matrix. 

$$
\begin{align*}
\sqrt{v_1} &= \sqrt{g_1^2} \\

&= \sqrt{diag(J^TJ)} && \text{Gauss-Newton approximation} \\

&\approx H && \text{Approximate Hessian matrix}

\end{align*}
$$

So, the element-wise square of the gradient is a crude diagonal approximation of the Hessian matrix. Knowing this, we can see that the Adagrad algorithm is a Newton's method with a diagonal approximation of the Hessian matrix for the first step.

Though, any reader might ask why Adagrad uses the cumulant, not the gradient itself. My guess is that due to the crude diagonal approximation of the Hessian matrix, we got a problem using the gradient itself. You can see:
$$
\begin{align*}
v_t &= g_t^2 && \text{Let's assume no cumulant} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t && \text{Update rule} \\

&= \theta_t - \eta \frac{g_t}{\sqrt{g_t^2 + \epsilon}} && \text{Substitute $v_t$} \\

&= \theta_t - \eta \, \text{sign}(g_t) && \text{As } \sqrt{g_t^2 + \epsilon} \approx |g_t| \text{ for small } \epsilon

\end{align*}
$$

...so, using instant gradient itself in the update rule collapses the optimization algorithm down to the sign-based update rule. I'm not saying that the gradient sign-based update rule is always bad, but it may not be flexible enough for many optimization problems.

Another fact we should appreciate is that usually we do not perform full-batch gradient descent but more of stochastic mini-batch gradient descent. In this case, the gradient at each step itself is a noisy estimate of the true gradient. If our update is small enough, we can mitigate the noise in the gradient with the cumulant.

Though, cumulant choice introduces a problem too. Adagrad is known to have a problem of the vanishing learning rate. The square of the gradient should be nonnegative, so the denominator of the update rule will increase over time.

### 3.2. RMSProp
RMSProp tries to mitigate the cumulant problem of the Adagrad algorithm by introducing a discount factor. The pseudocode of the RMSProp algorithm is as follows:

```python
discount_factor = 0.9
c_grad_square = 0

while True:
    ## Get the gradient of the loss function
    gradient = compute_gradient()

    ## Cumulant here is the element-wise square of the gradient
    ## Discount factor creates the effective moving window
    c_grad_square = discount_factor * c_grad_square + (1 - discount_factor) * gradient ** 2

    ## Update the parameter
    theta = theta - (learning_rate * gradient) / (np.sqrt(c_grad_square) + epsilon)
```
As the pseudocode shows, the update rule of the RMSProp algorithm is:
$$
\begin{align*}
g_t &= \nabla \mathcal{L}(\theta_t) && \text{Gradient of the loss function} \\
v_t &= \beta v_{t-1} + (1 - \beta) g_t^2 && \text{Cumulant of the gradient} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t && \text{Update rule}
\end{align*}
$$

Recall that $\beta = 0$ brings the sign-based update rule again. In many common practices, $\beta$ is set close to 1. Close-to-1 $\beta$ values works like momentum in the optimization algorithm. 

### 3.3. Adam
Finally, one of the most popular optimization algorithms, Adam, is an extension of the RMSProp algorithm. The pseudocode of the Adam algorithm is as follows:

```python
beta_grad = 0.9
beta_hess = 0.999 ## Shamelessly named square of the gradient as Hessian
c_grad = 0
c_grad_square = 0
t = 0

while True:
    ## Increment the time step
    t += 1  

    ## Get the gradient of the loss function
    gradient = compute_gradient()

    ## In Adam, we do EMA on the gradient too!
    c_grad = beta_grad * c_grad + (1 - beta_grad) * gradient
    c_grad_square = beta_hess * c_grad_square + (1 - beta_hess) * gradient ** 2

    ## Bias correction: explanation below
    c_grad = c_grad / (1 - beta_grad^t)
    c_grad_square = c_grad_square / (1 - beta_hess^t)

    ## Update the parameter
    theta = theta - (learning_rate * c_grad) / (np.sqrt(c_grad_square) + epsilon)
```
For the bias correction term, the intuition is this term mitigates the zero-initialization problem. For the first few step, the moving average of terms unnecessarily outweighs 0 if $\beta$ is close to 1. Introducing this term will mitigate this issue for the first few iteration. After enough iteration, you can see the bias correction term converges to 1, effectively doing nothing.


<br>
<br>

# 4. Reparameterization-invariant Optimization
TBD