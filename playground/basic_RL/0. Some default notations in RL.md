Some basic RL words, in a less rigorous way.

Agent
- You, or the machine, that interacts with the environment. 
- Agent takes $\textbf{actions}$ and observes what environment gives you back.

Environment
- The world. It can be a real world, a game, a control problem, etc.
- Usually, the environment gives you $\textbf{observations}$ and $\textbf{rewards}$.

State (env)
- The current situation of the environment.
- In some tasks, the state is directly observable. This is called fully observable environment.
- In others (i.e. real world), the state works as a latent factor. Agent only receives observations, which is a function of the state. This is called partially observable environment.
- In most cases, we assume the environment is $\textbf{Markovian}$, which means the current state contains enough information from the past states to predict future state.

Markov Decision Process (MDP)

Reward function
- An environment-side function that determines reward outcome.
- Usually denoted as $R: S \times A \to \mathbb{R}$. This means "given a state and an action, what reward will you get?"
- This form is also commonly used: $R: S \times A \times S \to \mathbb{R}$. This means "given a state, an action, and a next state that you arrive as a result of the action, what reward will you get?"
    - Expectation over the next state gets you the first form.

Policy
- A strategy that agent uses to choose actions.
- Usually denoted as a probability distribution: $\pi(a|s)$, or $\pi: S \to A$.

Value function

Model


Method for Prediction

Method for Control