# 1. Forgo Monte Carlo estimation for REINFORCE

Previously, we discussed the REINFORCE algorithm, which uses Monte Carlo estimation to estimate the policy gradient. By introducing a baseline, we mitigated the high variance of the policy gradient. We also briefly touched that the value function, if available, is a good candidate for the baseline.

However, we left with a question: how can we solve other problems come with Monte Carlo estimation? Even with the baseline, variance might still be high. Also, for the return to be available, we need to wait until the end of the episode. This is not ideal for many applications.

One idea to solve these problems is to find alternative ways to represent the return. Briefly, recall:

$$
\begin{align*}

G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots && \text{Discounted return} \\

\nabla J(\theta) &=  \sum_t \mathbb{E}_{\tau \sim \pi} \left[\gamma^t (G_t - b(s)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{REINFORCE} \\

v_\pi(s) &= \mathbb{E}_\pi \left[ G_t | s_t = s \right] && \text{Value function} \\

q_\pi(s, a) &= \mathbb{E}_\pi \left[ G_t | s_t = s, a_t = a \right] && \text{Action-value function} \\

\end{align*}
$$

Then, we can see where this is going. We can replace the actual return $G_t$ with a different estimate (which is called Bootstrap estimate in RL). 

First, we can replace the actual return $G_t$ with the action-value function $q_\pi(s, a)$:

$$
\begin{align*}
\tau &= (s_0, a_0, s_1, a_1, \ldots) && \text{state-action trajectory} \\

q_\pi(s, a) &= \mathbb{E}_{\tau \sim \pi} \left[ G_t | s_t = s, a_t = a \right] \\

\mathbb{E}_{\tau \sim \pi} \left[ G_t \nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ \mathbb{E} \left[ G_t | s_t, a_t\right] \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ q_\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{Q function form} \\

\end{align*}
$$

We have also learned that any action-independent function can be used as a baseline. So, we can subtract the value function $v_\pi(s)$ from the action-value function $q_\pi(s, a)$:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi} \left[ (G_t - b(s))\nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ (q_\pi(s_t, a_t) - v_\pi(s_t))\nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ A_\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{Advantage function} \\

\end{align*}
$$

Lastly, we can find out that the value function alone can be used to replace the return:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi} \left[ (G_t - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ (R_{t+1} + \gamma G_{t+1} - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ (R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ \delta_t \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{TD(0)} \\


\end{align*}
$$

In summary, we can replace the return with the other estimates. Theoretically (and, to our happiness, practically too), replacing the noise sample $G_t$ with other estimates achieves much lower variance. However, at that cost, we are inherently biased as we do not have the true action-value function or value function. 

<details>
    <summary>Pseudocode</summary>

blah blah

</details>


<br>
<br>

# 2. Generalized Advantage Estimation (GAE)
In the previous section, we discussed how to replace the return with other estimates to reduce variance. It is a natural question which estimate we should use. One of the most up-to-date and commonly used estimates is Generalized Advantage Estimation (GAE).

From the previous section, we can notice the two extreme alternative forms that approximate the advantage $A_\pi(s, a) = q_\pi(s, a) - v_\pi(s)$. 

1. The sampled return minus baseline $G_t - v_\pi(s_t)$, resembles TD(1) 
2. The one-step TD error $R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t)$, TD(0)

We can interpolate between these two extremes. First, let's think of the n-step TD error:

$$
\begin{align*}
\delta_t^{(1)} &= R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t) && \text{1-step TD error} \\
\delta_t^{(2)} &= R_{t+1} + \gamma R_{t+2} + \gamma^2 v_\pi(s_{t+2}) - v_\pi(s_t) && \text{2-step TD} \\
&= \delta_t^{(1)} + \gamma \delta_{t+1}^{(1)} && \text{recursive} \\
\delta_t^{(n)} &= \sum_{k=0}^{n-1} \gamma^k R_{t+k+1} + \gamma^n v_\pi(s_{t+n}) - v_\pi(s_t) && \text{n-step TD} \\
&= \sum_{k=0}^{n-1} \gamma^k \delta_t^{(1)} \\

\end{align*}
$$

Then, we can introduce another parameter $\lambda \in [0, 1]$ to interpolate between these two extremes. One simple interpolation choice would be an EMA, exponential moving average (this resembles TD($\lambda$)):
$$
\begin{align*}
\delta_t^{(\lambda)} &= (1-\lambda) (\delta_t^{(1)} + \lambda \delta_t^{(2)} + \lambda^2 \delta_t^{(3)} + \ldots) \\

&= (1-\lambda) \left[ \frac{\delta_t^{(1)}}{1-\lambda} + \frac{\gamma \lambda \delta_{t+1}^{(1)}}{1-\lambda} + \frac{\gamma^2 \lambda^2 \delta_{t+2}^{(1)}}{1-\lambda} + \ldots \right] \\

&= \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}^{(1)} && \text{GAE}\\

\end{align*}
$$

This average brought us to the TD($\lambda$)-inspired GAE($\gamma, \lambda$). As expected in TD($\lambda$), $\lambda$ in GAE also controls the bias-variance trade-off. When $\lambda = 0$, we have TD(0), which typically shows low variance but high bias. This is because TD(0) is only unbiased when we have a true value function $V_\pi$. On the contrary, $\lambda = 1$ gives us the REINFORCE with baseline method back, which typically shows high variance but no bias.

<details>
    <summary>Eligibility Trace?</summary>

TBD

</details>

<details>
    <summary>Other points on the paper...</summary>

TBD

</details>

<br>
<br>

# 3. Trust Region Policy Optimization (TRPO)
Until now, we have discussed how to improve the policy gradient estimation. From this section, we will discuss how to improve the policy update (i.e. can we do better than simple SGD?).

As you may already know, one way to improve the gradient update is to use second-order information. By using or approximating the Hessian matrix, we can utilize the Newton's method to find the optimal direction and step size, as we discussed [here](../Optimization/1.%20Newton's%20method%20and%20Adaptive%20Optimizers.md).

To our pleasant surprise, probability distributions have a nice property that the Fisher Information Matrix (FIM) is equivalent to the Hessian matrix of its KL divergence against infinitesimally nudged distribution. Moreover, this is also Gauss-Newton approximation. [Details are discussed here](../ELBO%20and%20VI/2.%20KL%20divergence%20rabbithole.md).

Therefore, we can plug in the Gauss-Newton update $H^{-1}g$ with $H$ as a Hessian matrix and $g$ as a policy gradient vector. To crudely say, this is the gist of the Natural Gradients and Natural Policy Gradients (NPG).

TRPO, indeed, practically does not go that far from NPG. Although the idea behind TRPO is pretty dense, the gist is to use the FIM (or the Hessian) to find the update direction and step size. Then, we put a hard constraint on the step size to avoid the big policy update. At this moment, one natural question might arise: as the NPG, or generally the second-order method, already provides the optimal step size, why do we even bother with the hard constraint on the step size? We will discuss this after we go through the TRPO algorithm.

The goal of TRPO is as follows:
$$
\begin{align*}

J(\theta) &= \sum_t \mathbb{E}_{\tau \sim \pi} \left[ A_t \ln \pi_\theta (a_t|s_t) \right] && \text{maximize the expected advantage} \\

\operatorname*{arg\max}_d J(\theta + d) \quad &\text{s.t.} \quad D_{KL}(\pi(\theta) || \pi(\theta + d)) \leq \delta && \text{Trust Region } \delta \\

\end{align*}
$$

The update rule of TRPO is as follows:
$$
\begin{align*}

\theta_{t+1} &= \theta_t + \alpha^j \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g && \text{TRPO update} \\

\end{align*}
$$

Now, how did we end up with this seemingly terrifying equation? Let's break it down.

First, we need to find the optimal update direction (or, search direction). This may rather obvious: $H^{-1} g$ is exactly it! However, there is one catch. Not like the standard Gauss-Newton situation, here the gradient vector $g$ is not the gradient of the policy itself but weighted by the advantage function. Again, $g = \sum_t \mathbb{E}_{\tau \sim \pi} \left[ A_\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right]$.

However, $H$ is the Hessian of the KL divergence between the original policy $\pi(\theta)$ and the nudged policy $\pi(\theta + \beta H^{-1} g)$. So, $H^{-1}g$ is not exactly the direction we achieve with Gauss-Newton!

Fortunately, this is still the right direction for the TRPO objective. Long story short, the Hessian matrix $H$ is $A_t$-away from the $\nabla_\theta^2 \mathbb{E}(J(\theta))$ (the Hessian of the objective function). So, we can still use this approximation. We will cover the details of this approximation in the dense discussion section below.

Then, we need to find the maximal step size $\beta$ such that the KL divergence between $\pi(\theta)$ and $\pi(\theta + \beta H^{-1} g)$ does not exceed our hard constraint $\delta$. Again, by second-order Taylor expansion and Fisher Information Matrix (FIM), we can find the following approximation:
$$
\begin{align*}

D_{KL}(\pi(\theta) || \pi(\theta + \beta H^{-1} g)) &\approx \frac{1}{2} (\beta g)^T H^{-1} (\beta g) && \text{FIM approximation} \\
&= \frac{1}{2} \beta^2 g^T H^{-1} g \\
&\leq \delta && \text{constraint} \\

\\

\beta &\leq \sqrt{\frac{2\delta}{g^T H^{-1} g}} && \text{maximal step size (NPG)} \\

\end{align*}
$$

So we recovered the terrifying term $\sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g$. This step is nicely pointed out by the NPG paper. Lastly, TRPO authors were being even more careful over NPG: they introduced a line search starting from maximal step size with $\alpha \in [0, 1]$ and $j=0$, then iteratively reduced the step size until the constraint is satisfied. This is why we have $\alpha^j$ in the update rule.

Now that we have a rough understanding of TRPO, let's get back to the question: why do we even bother with the hard constraint on the step size? I don't have an exact answer, but I can make some speculations.

First, not like supervised learning, the training dataset in RL is generated by the policy itself. This means that one big bad update can significantly warp the training data distribution, which potentially leads to the catastrophic failure. On the contrary, in supervised learning, the training dataset is fixed. So, I imagine that the network is more robust to the big update.

Second, our update rule is the collection of approximation and sampling. So, even though we have the optimal update size by theory, the actual sampled gradient might be noisy. Thus, the hard constraint on the step size might be able to compensate for this noise.

---

Some of you may ask - we have already throwed the step size calculated from the second-order method away. Why don't we just strip things down and use the first-order method like Adam? Adam is also a crude approximation of the second-order method.

Indeed, this is a valid question. Proximal Policy Optimization (PPO), which we will discuss next, is the answer to this question. PPO is a first-order method that can achieve comparable performance to TRPO.

<details>
    <summary>Dense discussion on TRPO</summary>

Okay, let's get nerdy. With the assumption that the update $d$ is small, we can use the first-order Taylor expansion for the $J(\theta + d)$ and second-order Taylor expansion for the KL divergence:

$$
\begin{align*}

&\operatorname*{arg\max}_d J(\theta + d) \quad \text{s.t.} \quad D_{KL}(\pi(\theta) || \pi(\theta + d)) \leq \delta && \text{TRPO Goal} \\

= &\operatorname*{arg\max}_d J(\theta + d) - \lambda D_{KL}(\pi(\theta) || \pi(\theta + d) - \delta) && \text{Lagrangian} \\

\approx &\operatorname*{arg\max}_d \left[ J(\theta_{\text{old}}) + \nabla_\theta J(\theta_{\text{old}})^T d \right] - \frac{\lambda}{2} d^T I(\theta_{\text{old}}) d - \lambda \delta && \text{Again, FIM} \\

\end{align*}
$$

Setting the gradient to zero, we have:
$$
\begin{align*}

0 &= \frac{\partial}{\partial d} \left[ \nabla_\theta J(\theta_{\text{old}})d - \frac{\lambda}{2} d^T I(\theta_{\text{old}}) d -\lambda \delta \right] \\

&= \nabla_\theta J(\theta_{\text{old}}) - \lambda I(\theta_{\text{old}}) d \\

d &= \frac{1}{\lambda} I(\theta_{\text{old}})^{-1} \nabla_\theta J(\theta_{\text{old}}) && \text{optimal update direction} \\

\end{align*}
$$

Substituting $I(\theta_{\text{old}})$ with $H$, we recover the update direction of TRPO (and NPG). Notice that we did not specify the step size yet!

---
You may ask, why don't we just use $\nabla_\theta^2 \mathbb{E}[J(\theta)]$? Although this might be more valid approach from the perspective of Gauss-Newton method, NPG / TRPO approach focuses more on the policy update in the parameter space. We can indeed see that $\nabla_\theta^2 \mathbb{E}[J(\theta)]$, with the trajectory and the advantage function collected from the old policy $\theta_{\text{old}}$, is $A_t \nabla_\theta^2 \pi_\theta(a_t|s_t)$. By dropping the $A_t$ term, we recover the NPG / TRPO Hessian.

Though, what did I mean by "the trajectory and the advantage function collected from the old policy"? For the details, we should look into the commonly used TRPO goal formulation below.

---
More commonly, you will see this TRPO goal formulation:
$$
\begin{align*}

\mathcal{L}(\theta_k, \theta) &= \mathbb{E}_{s, a \sim \pi_{\theta_k}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)} A(s, a) \right] \\

\operatorname*{max}_\theta \mathcal{L}(\theta_k, \theta) \quad &\text{s.t.} \quad D_{KL}(\pi_{\theta_k} || \pi_\theta) \leq \delta && \text{TRPO Goal, common form} \\

\end{align*}
$$

You can see that the loss function here is not exactly the same $\mathbb{E}[J(\theta)]$ but some kind of surrogate target. Now, this is confusing: I have told you that the gradient in TRPO update rule is just the policy gradient. However, with that surrogate target function, is it still the case?

Let's break it down. First, what is that surrogate target function? You may notice this is the importance sampling, commonly used in off-policy learning.

If we think about the sampling and update process, it is pretty straightforward: the trajectory and the advantage function are collected from the old policy $\theta_{\text{old}}$. Then, another way to think about the policy improvement problem is following:

$$
\begin{align*}

\sum_\tau \pi_\theta (\tau) A(\tau) &= \sum_\tau \pi_{\theta_{\text{old}}} (\tau) \frac{\pi_\theta (\tau)}{\pi_{\theta_{\text{old}}}(\tau)} A(\tau) && \text{Importance Sampling} \\

&= \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} A_\pi(s_t, a_t) \right] && \text{Surrogate target} \\

\end{align*}
$$

So, the surrogate target means that we evaluate the new policy $\pi_\theta$ with the data collected from the old policy $\pi_{\theta_{\text{old}}}$.

Then, as $\pi_{\theta_{\text{old}}}$ is fixed, we can take the gradient with respect to $\theta$:
$$
\begin{align*}

\nabla_\theta \mathcal{L}(\theta_{\text{old}}, \theta) &= \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ \nabla_\theta \left( \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right) \right] \\

&= \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\nabla_\theta \pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A(s, a) \right] \\

&\approx \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ A(s, a) \nabla_\theta \log \pi_\theta(a|s) \right] && \text{Assume small change} \\

\end{align*}
$$

Therefore, upon the assumption that $\theta$ and $\theta_{\text{old}}$ are close enough, we can say that the gradient of the surrogate target is approximately the same as the policy gradient. This is why we can still use the policy gradient in the TRPO update rule.

One last thing to note is that the importance sampling might introduce high variance. This is due to the term:
$$
\frac{\pi_\theta(\tau)}{\pi_{\theta_{\text{old}}}(\tau)} = \prod_{t=0}^{T-1} \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$

This is a product of ratios, which can be very large or very small. If the new policy $\pi_\theta$ is too different from the old policy $\pi_{\theta_{\text{old}}}$, this term can either explode or vanish. I imagine the hard trust region constraint $\delta$ in NPG tries to mitigate this problem, and TRPO further literally checks this term with the line search $\alpha^j$.


</details>
<details>
    <summary>Conjugate Gradient?</summary>

TBD

</details>

<br>
<br>

# 4. Proximal Policy Optimization (PPO)
In this section, we will discuss how to improve the policy update without using second-order information. 