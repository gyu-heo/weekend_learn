# 1. Forgo Monte Carlo estimation for REINFORCE

Previously, we discussed the REINFORCE algorithm, which uses Monte Carlo estimation to estimate the policy gradient. By introducing a baseline, we mitigated the high variance of the policy gradient. We also briefly touched that the value function, if available, is a good candidate for the baseline.

However, we left with a question: how can we solve other problems come with Monte Carlo estimation? Even with the baseline, variance might still be high. Also, for the return to be available, we need to wait until the end of the episode. This is not ideal for many applications.

One idea to solve these problems is to find alternative ways to represent the return. Briefly, recall:

$$
\begin{align*}

G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots && \text{Discounted return} \\

\nabla J(\theta) &=  \sum_t \mathbb{E}_{\tau \sim \pi} \left[\gamma^t (G_t - b(s)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{REINFORCE} \\

v_\pi(s) &= \mathbb{E}_\pi \left[ G_t | s_t = s \right] && \text{Value function} \\

q_\pi(s, a) &= \mathbb{E}_\pi \left[ G_t | s_t = s, a_t = a \right] && \text{Action-value function} \\

\end{align*}
$$

Then, we can see where this is going. We can replace the actual return $G_t$ with a different estimate (which is called Bootstrap estimate in RL). 

First, we can replace the actual return $G_t$ with the action-value function $q_\pi(s, a)$:

$$
\begin{align*}
\tau &= (s_0, a_0, s_1, a_1, \ldots) && \text{state-action trajectory} \\

q_\pi(s, a) &= \mathbb{E}_{\tau \sim \pi} \left[ G_t | s_t = s, a_t = a \right] \\

\mathbb{E}_{\tau \sim \pi} \left[ G_t \nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ \mathbb{E} \left[ G_t | s_t, a_t\right] \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ q_\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{Q function form} \\

\end{align*}
$$

We have also learned that any action-independent function can be used as a baseline. So, we can subtract the value function $v_\pi(s)$ from the action-value function $q_\pi(s, a)$:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi} \left[ (G_t - b(s))\nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ (q_\pi(s_t, a_t) - v_\pi(s_t))\nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ A_\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{Advantage function} \\

\end{align*}
$$

Lastly, we can find out that the value function alone can be used to replace the return:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi} \left[ (G_t - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ (R_{t+1} + \gamma G_{t+1} - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ (R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ \delta_t \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{TD(0)} \\


\end{align*}
$$

In summary, we can replace the return with the other estimates. Theoretically (and, to our happiness, practically too), replacing the noise sample $G_t$ with other estimates achieves much lower variance. However, at that cost, we are inherently biased as we do not have the true action-value function or value function. 

<details>
    <summary>Pseudocode</summary>

blah blah

</details>


<br>
<br>

# 2. Generalized Advantage Estimation (GAE)
In the previous section, we discussed how to replace the return with other estimates to reduce variance. It is a natural question which estimate we should use. One of the most up-to-date and commonly used estimates is Generalized Advantage Estimation (GAE).

From the previous section, we can notice the two extreme alternative forms that approximate the advantage $A_\pi(s, a) = q_\pi(s, a) - v_\pi(s)$. 

1. The sampled return minus baseline $G_t - v_\pi(s_t)$, resembles TD(1) 
2. The one-step TD error $R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t)$, TD(0)

We can interpolate between these two extremes. First, let's think of the n-step TD error:

$$
\begin{align*}
\delta_t^{(1)} &= R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t) && \text{1-step TD error} \\
\delta_t^{(2)} &= R_{t+1} + \gamma R_{t+2} + \gamma^2 v_\pi(s_{t+2}) - v_\pi(s_t) && \text{2-step TD} \\
&= \delta_t^{(1)} + \gamma \delta_{t+1}^{(1)} && \text{recursive} \\
\delta_t^{(n)} &= \sum_{k=0}^{n-1} \gamma^k R_{t+k+1} + \gamma^n v_\pi(s_{t+n}) - v_\pi(s_t) && \text{n-step TD} \\
&= \sum_{k=0}^{n-1} \gamma^k \delta_t^{(1)} \\

\end{align*}
$$

Then, we can introduce another parameter $\lambda \in [0, 1]$ to interpolate between these two extremes. One simple interpolation choice would be an EMA, exponential moving average (this resembles TD($\lambda$)):
$$
\begin{align*}
\delta_t^{(\lambda)} &= (1-\lambda) (\delta_t^{(1)} + \lambda \delta_t^{(2)} + \lambda^2 \delta_t^{(3)} + \ldots) \\

&= (1-\lambda) \left[ \frac{\delta_t^{(1)}}{1-\lambda} + \frac{\gamma \lambda \delta_{t+1}^{(1)}}{1-\lambda} + \frac{\gamma^2 \lambda^2 \delta_{t+2}^{(1)}}{1-\lambda} + \ldots \right] \\

&= \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}^{(1)} && \text{GAE}\\

\end{align*}
$$

This average brought us to the TD($\lambda$)-inspired GAE($\gamma, \lambda$). As expected in TD($\lambda$), $\lambda$ in GAE also controls the bias-variance trade-off. When $\lambda = 0$, we have TD(0), which typically shows low variance but high bias. This is because TD(0) is only unbiased when we have a true value function $V_\pi$. On the contrary, $\lambda = 1$ gives us the REINFORCE with baseline method back, which typically shows high variance but no bias.

<details>
    <summary>Eligibility Trace?</summary>

TBD

</details>

<details>
    <summary>Other points on the paper...</summary>

TBD

</details>

<br>
<br>

# 3. Trust Region Policy Optimization (TRPO)
TBD