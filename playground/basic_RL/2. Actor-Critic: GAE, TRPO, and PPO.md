# 1. Forgo Monte Carlo estimation for REINFORCE

Previously, we discussed the REINFORCE algorithm, which uses Monte Carlo estimation to estimate the policy gradient. By introducing a baseline, we mitigated the high variance of the policy gradient. We also briefly touched that the value function, if available, is a good candidate for the baseline.

However, we left with a question: how can we solve other problems come with Monte Carlo estimation? Even with the baseline, variance might still be high. Also, for the return to be available, we need to wait until the end of the episode. This is not ideal for many applications.

One idea to solve these problems is to find alternative ways to represent the return. Briefly, recall:

$$
\begin{align*}

G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots && \text{Discounted return} \\

\nabla J(\theta) &=  \sum_t \mathbb{E}_{\tau \sim \pi} \left[\gamma^t (G_t - b(s)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{REINFORCE} \\

v_\pi(s) &= \mathbb{E}_\pi \left[ G_t | s_t = s \right] && \text{Value function} \\

q_\pi(s, a) &= \mathbb{E}_\pi \left[ G_t | s_t = s, a_t = a \right] && \text{Action-value function} \\

\end{align*}
$$

Then, we can see where this is going. We can replace the actual return $G_t$ with a different estimate (which is called Bootstrap estimate in RL). 

First, we can replace the actual return $G_t$ with the action-value function $q_\pi(s, a)$:

$$
\begin{align*}
\tau &= (s_0, a_0, s_1, a_1, \ldots) && \text{state-action trajectory} \\

q_\pi(s, a) &= \mathbb{E}_{\tau \sim \pi} \left[ G_t | s_t = s, a_t = a \right] \\

\mathbb{E}_{\tau \sim \pi} \left[ G_t \nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ \mathbb{E} \left[ G_t | s_t, a_t\right] \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ q_\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{Q function form} \\

\end{align*}
$$

We have also learned that any action-independent function can be used as a baseline. So, we can subtract the value function $v_\pi(s)$ from the action-value function $q_\pi(s, a)$:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi} \left[ (G_t - b(s))\nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ (q_\pi(s_t, a_t) - v_\pi(s_t))\nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ A_\pi(s_t, a_t) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{Advantage function} \\

\end{align*}
$$

Lastly, we can find out that the value function alone can be used to replace the return:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi} \left[ (G_t - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] &= \mathbb{E}_{\tau \sim \pi} \left[ (R_{t+1} + \gamma G_{t+1} - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ (R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\

&= \mathbb{E}_{\tau \sim \pi} \left[ \delta_t \nabla_\theta \log \pi_\theta(a_t|s_t) \right] && \text{TD(0)} \\


\end{align*}
$$

In summary, we can replace the return with the other estimates. Theoretically (and, to our happiness, practically too), replacing the noise sample $G_t$ with other estimates achieves much lower variance. However, at that cost, we are inherently biased as we do not have the true action-value function or value function. 

<details>
    <summary>Pseudocode</summary>

blah blah

</details>


<br>
<br>

# 2. Generalized Advantage Estimation (GAE)
In the previous section, we discussed how to replace the return with other estimates to reduce variance. It is a natural question which estimate we should use. One of the most up-to-date and commonly used estimates is Generalized Advantage Estimation (GAE).

From the previous section, we can notice the two extreme alternative forms that approximate the advantage $A_\pi(s, a) = q_\pi(s, a) - v_\pi(s)$. 

1. The sampled return minus baseline $G_t - v_\pi(s_t)$, resembles TD(1) 
2. The one-step TD error $R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t)$, TD(0)

We can interpolate between these two extremes. First, let's think of the n-step TD error:

$$
\begin{align*}
\delta_t^{(1)} &= R_{t+1} + \gamma v_\pi(s_{t+1}) - v_\pi(s_t) && \text{1-step TD error} \\
\delta_t^{(2)} &= R_{t+1} + \gamma R_{t+2} + \gamma^2 v_\pi(s_{t+2}) - v_\pi(s_t) && \text{2-step TD} \\
&= \delta_t^{(1)} + \gamma \delta_{t+1}^{(1)} && \text{recursive} \\
\delta_t^{(n)} &= \sum_{k=0}^{n-1} \gamma^k R_{t+k+1} + \gamma^n v_\pi(s_{t+n}) - v_\pi(s_t) && \text{n-step TD} \\
&= \sum_{k=0}^{n-1} \gamma^k \delta_t^{(1)} \\

\end{align*}
$$

Then, we can introduce another parameter $\lambda \in [0, 1]$ to interpolate between these two extremes. One simple interpolation choice would be an EMA, exponential moving average (this resembles TD($\lambda$)):
$$
\begin{align*}
\delta_t^{(\lambda)} &= (1-\lambda) (\delta_t^{(1)} + \lambda \delta_t^{(2)} + \lambda^2 \delta_t^{(3)} + \ldots) \\

&= (1-\lambda) \left[ \frac{\delta_t^{(1)}}{1-\lambda} + \frac{\gamma \lambda \delta_{t+1}^{(1)}}{1-\lambda} + \frac{\gamma^2 \lambda^2 \delta_{t+2}^{(1)}}{1-\lambda} + \ldots \right] \\

&= \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}^{(1)} && \text{GAE}\\

\end{align*}
$$

This average brought us to the TD($\lambda$)-inspired GAE($\gamma, \lambda$). As expected in TD($\lambda$), $\lambda$ in GAE also controls the bias-variance trade-off. When $\lambda = 0$, we have TD(0), which typically shows low variance but high bias. This is because TD(0) is only unbiased when we have a true value function $V_\pi$. On the contrary, $\lambda = 1$ gives us the REINFORCE with baseline method back, which typically shows high variance but no bias.

<details>
    <summary>Eligibility Trace?</summary>

TBD

</details>

<details>
    <summary>Other points on the paper...</summary>

TBD

</details>

<br>
<br>

# 3. Trust Region Policy Optimization (TRPO)
Until now, we have discussed how to improve the policy gradient estimation. From this section, we will discuss how to improve the policy update (i.e. can we do better than simple SGD?).

As you may already know, one way to improve the gradient update is to use second-order information. By using or approximating the Hessian matrix, we can utilize the Newton's method to find the optimal direction and step size, as we discussed [here](../Optimization/1.%20Newton's%20method%20and%20Adaptive%20Optimizers.md).

To our pleasant surprise, probability distributions have a nice property that the Fisher Information Matrix (FIM) is equivalent to the Hessian matrix of its KL divergence against infinitesimally nudged distribution. Moreover, this is also Gauss-Newton approximation. [Details are discussed here](../ELBO%20and%20VI/2.%20KL%20divergence%20rabbithole.md).

Therefore, we can plug in the Gauss-Newton update $H^{-1}g$ with $H$ as a Hessian matrix and $g$ as a gradient vector. To crudely say, this is the gist of the Natural Gradients and Natural Policy Gradients (NPG).

TRPO, indeed, practically does not go that far from NPG. Although the idea behind TRPO is pretty dense, the gist is to use the FIM (or the Hessian) to find the update direction and step size. Then, we put a hard constraint on the step size to avoid the big policy update. At this moment, one natural question might arise: as the NPG, or generally the second-order method, already provides the optimal step size, why do we even bother with the hard constraint on the step size? We will discuss this after we go through the TRPO algorithm.

The update rule of TRPO is as follows:
$$
\begin{align*}

\theta_{t+1} &= \theta_t + \alpha^j \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g && \text{TRPO update} \\

\end{align*}
$$

Now, how did we end up with this seemingly terrifying equation? Let's break it down.

First, we need to find the optimal update direction (or, search direction). This is rather obvious: $H^{-1} g$ is exactly it!

Then, we need to find the maximal step size $\beta$ such that the KL divergence between $\pi(\theta)$ and $\pi(\theta + \beta H^{-1} g)$ does not exceed our hard constraint $\delta$. Again, by second-order Taylor expansion and Fisher Information Matrix (FIM), we can find the following approximation:
$$
\begin{align*}

D_{KL}(\pi(\theta) || \pi(\theta + \beta H^{-1} g)) &\approx \frac{1}{2} (\beta g)^T H^{-1} (\beta g) && \text{FIM approximation} \\
&= \frac{1}{2} \beta^2 g^T H^{-1} g \\
&\leq \delta && \text{constraint} \\

\\

\beta &\leq \sqrt{\frac{2\delta}{g^T H^{-1} g}} && \text{maximal step size} \\

\end{align*}
$$

So we recovered the terrifying term $\sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g$. Lastly, TRPO authors were being even more careful: they introduced a line search starting from maximal step size with $\alpha \in [0, 1]$ and $j=0$, then iteratively reduced the step size until the constraint is satisfied. This is why we have $\alpha^j$ in the update rule.

Now that we have a rough understanding of TRPO, let's get back to the question: why do we even bother with the hard constraint on the step size? I don't have an exact answer, but I can make some speculations.

First, not like supervised learning, the training dataset in RL is generated by the policy itself. This means that one big bad update can significantly warp the training data distribution, which potentially leads to the catastrophic failure. On the contrary, in supervised learning, the training dataset is fixed. So, I imagine that the network is more robust to the big update.

Second, our update rule is the collection of approximation and sampling. So, even though we have the optimal update size by theory, the actual sampled gradient might be noisy. Thus, the hard constraint on the step size might be able to compensate for this noise.

---

Some of you may ask - we have already throwed the step size calculated from the second-order method away. Why don't we just strip things down and use the first-order method like Adam? Adam is also a crude approximation of the second-order method.

Indeed, this is a valid question. Proximal Policy Optimization (PPO), which we will discuss next, is the answer to this question. PPO is a first-order method that can achieve comparable performance to TRPO.

<details>
    <summary>Dense discussion on TRPO</summary>

TBD

</details>
<details>
    <summary>Conjugate Gradient?</summary>

TBD

</details>

<br>
<br>

# 4. Proximal Policy Optimization (PPO)
In this section, we will discuss how to improve the policy update without using second-order information. 