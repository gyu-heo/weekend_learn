One weekend, you finally make a choice: you decide to fly to Vegas and sit in front of a slot machine. You know you must be cautious with your actions — otherwise, Vegas will eat you alive. You wonder which action to take. Should you pull the first lever or the second? Spoiler alert: the best answer might be, "Just leave the machine."

The key point of this story is simple. As a reinforcement learning (RL) agent, your only way to interact with the environment is through actions. Wherever you go, you would love to learn the optimal action to take in each situation. That’s the essence of a policy: Map states to actions. Though, the question remains: How do you learn the policy?

Today, we will explore the motivation behind the REINFORCE algorithm and connect it to other general learning rules such as node perturbation. After that, we will informally expand REINFORCE to more advanced methods (i.e. actor-critic), but the details will be covered in later notebooks.

<details>
    <summary>Some excuses...</summary>


Many RL sources introduce REINFORCE later, after discussing value-based methods. I believe this order is for the rigorous approach. By first going through value-based methods, you get familiar with the concepts such as action-value, Monte Carlo, temporal difference learning, and so on. These concepts are, although not directly, the building blocks of policy-based methods.

Here, we will sacrifice this canonical order to introduce policy-based methods and REINFORCE first. This could be a bad approach...but for me, start from answering "Isn't all you need the policy, which dictates what to do?" feels more natural.

</details>

<br>
<br>

## 1. What is the REINFORCE algorithm?

Let's say we have a prior $\textbf{policy function}$, $\pi$, which maps states $S$ (or observations) to a $\textbf{probability distribution}$ over actions $A$. In other words, $\pi: S\times A \to [0,1]$. Ideally, we would like to discover an optimal policy function $\pi^*$ that maximizes returns in any environment.

But how can we improve our $\pi$? One natural approach, when $\pi$ is differentiable, is to update $\pi$ directly using experience and gradient-based optimization.



### 1.1. Real-time (immediate) discrete case

Let's consider the simplest scenario: a two-armed bandit task. You are sitting in front of a slot machine with multiple, or two, arms (i.e. Bandits, lol).

All arms look same to you: so, there is no prior "clue" or evidence for you to know which arm gives you the best reward. All you have is a $\textbf{discrete}$ action space to decide which arm to pull. The reward is received immediately after the action. Then, the episode terminates.

We start by parameterizing our policy with a parameter set $\theta$. This can be anything - some predefined function, a neural network, etc. For every episode, we would like to update $\theta$ so that we can maximize the reward from the bandit.

Then, for each iteration (or episode) $i$, we can simply start with a gradient ascent approach (since we are maximizing the expected reward). For any action $a \in A$:

$$
\text{Goal:} \quad \operatorname*{arg\,max}_\theta \mathbb{E}_{a\sim \pi_\theta}[R(a)] \\
\theta_{i+1} = \theta_{i} + \eta \nabla_\theta \mathbb{E}_{a\sim \pi_{\theta}}[R(a)] \\
$$

...where $\eta$ is a learning rate. $R$ is the reward function $R: S\times A \to \mathbb{R}$, as it maps (state, action) pair to the reward given. Notice that our initial state distribution is degenerate here: every episode starts in exactly the same way. Therefore, we can marginalize state out, leaving $R(a)$.

Okay, then how do we proceed with this derivation over expectation?

$$
\begin{align*}
\nabla_\theta \mathbb{E}_{a\sim \pi_{\theta}}[R(a)]

&= \nabla_\theta \sum_a \pi_{\theta}(a) \mathbb{E}_{r\sim p(R|a)}[r] && \text{Law of total expectation} \\

&= \nabla_\theta \sum_a \pi_{\theta}(a)q(a) && (q(a) \triangleq \mathbb{E}[R(a)|a]) \\

&= \sum_a \nabla_\theta \pi_{\theta}(a)q(a) && \text{Leibniz Integral Rule} \\
\end{align*}
$$

$q(s,a)$ is an action value function where $q: S \times A \to \mathbb{R}$. Again, we marginalized state out in this environment. $q(a)$ here handles the stochasticity of the reward function. If the reward function is deterministic (i.e., the arm's reward probability is 1), then the Law of Total Expectation, or $q(a)$, simply reduces to $R(a)$.

If we have access to the true $q(a)$ or $R(a)$, we can compute for exact gradient ascent. However, in most cases, we don't. Fortunately, we can use sampled values to replace expectations, shifting from exact gradient ascent to stochastic gradient ascent.

Let's take a few more steps to convert everything back to expectation form:


$$
\begin{align*}
\nabla_\theta \mathbb{E}_{a\sim \pi_{\theta}}[R(a)]

&= \sum_a \nabla_\theta \pi_{\theta}(a)q(a) && \text{Leibniz Integral Rule; cont'd} \\

&= \sum_a q(a) \frac{\pi_{\theta}(a)}{\pi_{\theta}(a)} \nabla_\theta \pi_{\theta}(a) \\

&= \sum_a q(a) \pi_{\theta}(a) \nabla_\theta \ln \pi_{\theta}(a) && \text{Log derivative trick} \\

&= \mathbb{E}_{a\sim\pi_{\theta}}[q(a) \nabla_\theta \ln \pi_{\theta}(a)]
\end{align*}
$$

Yes! Now we sample $a_i$ and observe (sampled) $r_i$ at each episode to replace $q(a)$. This is again justified as $q(a)$ by itself is an expectation $\mathbb{E}[R(a)|a]$ as we defined earlier. Now we are ready to estimate the gradient:

Our stochastic gradient ascent approach, then, follows:

$$
\theta_{i+1} = \theta_{i} + \eta r_i \nabla_\theta \ln \pi_{\theta_{i}}(a_i)
$$

---

Nice! Now, let's assume our action space is small and discrete. Can we make a tangible policy parameterization case? 

We can define an action preference function $H(a)$ for each action $a \in A$. This is $\textbf{not a value function}$ but rather represents the unnormalized preference for selecting a particular action. Using this, we can define our policy over the discrete action space as follows:

$$
\pi_\theta(a) \triangleq \frac{\exp(H(a))}{\sum_b \exp(H(b))} = \text{SOFTMAX}(H)_a
$$

As you may have noticed, here we replaced the general parameter set $\theta$ with our action preference function set $H$. Then, for each iteration (or episode) $i$, the equation simply boils down to:

$$
\begin{align*}
\nabla_\theta \ln \pi_{\theta_{i}}(a_i) &= \nabla_\theta  H_i(a_i) - \nabla_\theta \ln \sum_b  \exp(H_i(b)) \\

&= \nabla_\theta  H_i(a_i) - \frac{\sum_b \exp(H_i(b))\nabla_\theta H_i(b)}{\sum_b \exp(H_i(b))} \\

&= \nabla_\theta  H_i(a_i) - \sum_b \pi_{\theta_i}(b) \nabla_\theta H_i(b)
\end{align*}
$$

...and let's plug this back in our stochastic gradient ascent equation.

$$
\begin{align*}
H_{i+1}(a) &= H_i(a) + \eta r_i \frac{\partial \ln \pi_{\theta_{i}}(a_i)}{\partial H_i(a)} \\

&= H_i(a) + \eta r_i \frac{\partial (H_i(a_i) - \ln \sum_b  \exp(H_i(b)))}{\partial H_i(a)} \\

&= H_i(a) + \eta r_i (I(a=a_i) - \pi_{\theta_{i}}(a)) \\

&= \begin{cases}
    H_i(a) + \eta r_i (1 - \pi_{\theta_{i}}(a)) & \text{if } a = a_i \\
    H_i(a) - \eta r_i \pi_{\theta_{i}}(a) & \text{else}
\end{cases}
\end{align*}
$$

...so you can see the gradient indeed involves every discrete action in $A$, not only the action taken at current episode. When the taken action $a_i$ coincides with the reward, REINFORCE not only increases the action preference for $a_i$ but decreases the action preference for every other action.

<details>
    <summary> The action preference function is also an expectation... </summary>


Sampling-based gradient approximation naturally raises an idea: Rather than focusing on estimating the policy gradient $\nabla_\theta$ or action preference $\nabla_H$, one could instead approximate the action-value function $q(s,a) = \mathbb{E}[R(s,a)|s,a]$ with another function $Q(s,a)$. In contrast to policy gradient methods, this approach leads to value-based learning methods such as Q-learning, which updates $Q(s,a)$ iteratively rather than relying on sampled policy gradients. We will deal with this much later.

</details>

<details>
    <summary> Can we do better than SGD...? </summary>


In the above derivation, we used a simple stochastic gradient ascent approach. Though, in practice, we rarely use pure SGD to train a neural network. You may ask, to better approximate the policy,

1. Can we use a second-order optimization method, such as Newton's method?
2. Can we use a more advanced optimizer, such as Adam?

These are, indeed, brilliant questions that lead to the development of the method called "Natural Policy Gradient" and "TRPO" (Trust Region Policy Optimization). Feel free to forget about these for now, as we will cover them in later sections.

</details>

<details>
    <summary> Pseudocode </summary>


```python

## REINFORCE iteration
while True:
    ## Generate an episode, following current policy
    (action, reward) = interact_with_bandit(policy_params)

    ## Update action preference function
    policy_params += learning_rate * reward * calculate_policy_gradient(action, policy_params)

    ## Check for convergence
    if converged(policy_params_history):
        break

```

</details>

<br>
<br>

### 1.2. Delayed (series-of-action), discrete case

In bandit task, we did not consider state. So, let's expand our understanding to the world where our action space is still discrete but episode terminates after series of actions (i.e. trajectories).

One nice example of this is the cliff walking environment: it is a variant of the gridworld. You are sitting in a maze: every move may incur some cost (like, negative rewards), and falling off the cliff costs you tons. Only reaching to the terminal grid gets you some positive rewards.

As you may have noticed, this is a good but surprisingly harder extension of the bandit problem: you should solve a credit assignment problem where you should correctly "credit" the action taken long before you get a reward (or punishment).

As we have introduced the concept of "different steps", allow me to slightly abuse the notation: in our previous section we used subscript to denote the iteration, but now we will use subscript to denote the time step IN this iteration.

---

For the multi-step case, let's define a few more notations. First, we define the return $G_t$ at time $t$ as the sum of rewards from time $t$ to the end of the episode $G_t = R_{t+1} + R_{t+2} + \ldots + R_T$. Surely, this definition only applies to the environment that terminates - but for now, let's assume our environment is episodic.

Why did we introduced the return? Because, now you should not be too myopic: as what you do at time $t$ may affect the return at time $T$, your objective is now to maximize the expected return, not the immediate reward.

Then, let's define the trajectory $\tau = (s_0, a_0, s_1, a_1, \ldots, s_T)$ as the sequence of states and actions from the start to the end of the episode. We can define the probability of $\tau$ as $\pi_\theta(\tau) = p(s_0) \pi_\theta(a_0|s_0) p(s_1|s_0, a_0) \ldots p(s_T|s_{T-1}, a_{T-1})$. Note that the distribution of the initial state $p(s_0)$ and the transition dynamics $p(s_{t+1}|s_t, a_t)$ are the environment function, and unknown to the agent at this moment.

Following this, we can define our objective function and proceed with the trajectory $\tau$ instead of the single action in the previous section:

$$
\begin{align*}
G(\tau) &= \sum^T_{t=1} R_t && G_0 \\

J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta}[G(\tau)] && \text{Objective function} \\

\nabla_\theta J(\theta) &= \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[G(\tau)] \\

&= \nabla_\theta \sum_\tau \pi_\theta(\tau) G(\tau) \\

&= \sum_\tau G(\tau) \nabla_\theta \pi_\theta(\tau) && \text{Leibniz Integral Rule} \\

&= \sum_\tau G(\tau) \pi_\theta(\tau) \nabla_\theta \ln \pi_\theta(\tau) && \text{Again, log derivative trick} \\

&= \mathbb{E}_{\tau \sim \pi_\theta}[G(\tau) \nabla_\theta \ln \pi_\theta(\tau)] \\

&= \mathbb{E}_{\tau \sim \pi_\theta}\left[G(\tau) \sum^{T-1}_{t=0} \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] && \text{Expand trajectory probability} \\

&= \sum^{T-1}_{t=0} \mathbb{E}_{\tau \sim \pi_\theta}\left[G(\tau) \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] && \text{Swap summation order} \\

\end{align*}
$$

...where the trajectory probability expansion follows simple log rules:

$$
\begin{align*}

\ln \pi_\theta(\tau) &= \ln \left[ p(s_0)\pi_\theta(a_0|s_0)p(s_1|s_0, a_0) \ldots p(s_T|s_{T-1}, a_{T-1}) \right] \\

&= \ln p(s_0) + \ln \pi_\theta(a_0|s_0) + \ln p(s_1|s_0, a_0) + \ldots + \ln p(s_T|s_{T-1}, a_{T-1}) \\

&= \ln p(s_0) + \sum^{T-1}_{t=0} \ln \pi_\theta(a_t|s_t) + \sum^{T-1}_{t=0} \ln p(s_{t+1}|s_t, a_t) \\

\end{align*}
$$

Interestingly, you can notice the time dependency in the gradient! Any reward taken BEFORE any action $a_t$ should be independent of the action $a_t$. For a trajectory $(s_0, a_0, r_1, s_1, a_1, r_2, ...)$, $r_1$ only depends on $(s_0, a_0)$. Can we apply this to our gradient estimation?

Note that, here I keep the discrete sum notion, but we can easily convert this to the continuous case by replacing the sum with the integral. With the simplest case of $t=1$, we can prototype this idea:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi_\theta}\left[R_1 \nabla_\theta \ln \pi_\theta(a_1|s_1)\right] &= \sum_{\tau} \pi_\theta(\tau)R_1 \nabla_\theta \ln \pi_\theta(a_1|s_1) \\
&= \sum_{\tau_{:1}}\sum_{a_1}\sum_{\tau_{1:}}\pi_\theta(\tau_{:1})\pi_\theta(a_1|s_1)\pi_\theta(\tau_{1:})R_1 \nabla_\theta \ln \pi_\theta(a_1|s_1) && \text{abused notation} \\

&= \sum_{\tau_{:1}}\sum_{a_1} \pi_\theta(\tau_{:1})\pi_\theta(a_1|s_1)R_1 \nabla_\theta \ln \pi_\theta(a_1|s_1) \sum_{\tau_{1:}}\pi_\theta(\tau_{1:}) && \text{Reorder summation} \\

&= \sum_{\tau_{:1}}\sum_{a_1} \pi_\theta(\tau_{:1})\pi_\theta(a_1|s_1)R_1 \nabla_\theta \ln \pi_\theta(a_1|s_1)  && \text{Sum probability to 1} \\

&= \sum_{\tau_{:1}} \pi_\theta(\tau_{:1})R_1 \sum_{a_1}\pi_\theta(a_1|s_1) \nabla_\theta \ln \pi_\theta(a_1|s_1)  && \text{Reorder summation} \\

&= \sum_{\tau_{:1}} \pi_\theta(\tau_{:1})R_1 \nabla_\theta \sum_{a_1}\pi_\theta(a_1|s_1) \ln \pi_\theta(a_1|s_1)  && \text{Leibniz Integral Rule} \\

&= \sum_{\tau_{:1}} \pi_\theta(\tau_{:1})R_1 \nabla_\theta 1  && \text{Expectation of score function} \\

&= 0

\end{align*}
$$

Therefore,

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi_\theta}\left[G(\tau) \nabla_\theta \ln \pi_\theta(a_1|s_1)\right] &= \mathbb{E}_{\tau \sim \pi_\theta}\left[R_1 \nabla_\theta \ln \pi_\theta(a_1|s_1)\right] + \mathbb{E}_{\tau \sim \pi_\theta}\left[G_1 \nabla_\theta \ln \pi_\theta(a_1|s_1)\right] \\

&= \mathbb{E}_{\tau \sim \pi_\theta}\left[G_1 \nabla_\theta \ln \pi_\theta(a_1|s_1)\right] \\

\end{align*}
$$

Generalize this to the whole trajectory, our gradient estimation finally arrives at vanilla REINFORCE implementation:

$$
\begin{align*}

\nabla_\theta J(\theta) &= \sum^{T-1}_{t=0} \mathbb{E}_{\tau \sim \pi_\theta}\left[G_t \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] \\

\theta_{t+1} &= \theta_{t} + \eta G_t \nabla_\theta \ln \pi_\theta(a_t|s_t) && \text{NOT on-the-fly update} \\

\end{align*}
$$

Beware - although we wrote the final update line as if we update $\theta$ every time step on-the-fly, this is not the case. Remember, the value $G_t$ is not known until the end of the episode. Therefore, we should accumulate the gradient over the whole episode and update $\theta$ at the end of the episode. This is the reason why REINFORCE is also known as "Monte Carlo policy gradient".

<details>
    <summary> Pseudocode </summary>

```python
## Undiscounted REINFORCE iteration
while True:
    ## Generate an episode, following current policy
    ## trajectory = [(s_0, a_0, r_1), (s_1, a_1, r_2), ...]
    ## reward_traj = [r_1, r_2, ...]
    (trajectory, reward_traj) = interact_with_environment(policy_params)

    ## Iterate over (s_t, a_t) pairs
    for t, (s_t, a_t, _) in enumerate(trajectory):
        ## Calculate the return G_t
        G_t = sum(reward_traj[t:])

        ## Update policy
        policy_params += learning_rate * G_t * calculate_pg(a_t, s_t, policy_params)

        ## calculate_pg = \nabla_\theta \ln \pi_\theta(a_t|s_t)
```
</details>

<details>
    <summary> Discount factor? </summary>

The discount factor $\gamma$ can be easily incorporated into the return $G_t$ as $G_t = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{T-t-1}R_T$. It is easy to notice $G_0 = R_1 + \gamma G_1 = R_1 + \gamma R_2 + \gamma^2 G_3 = \ldots $

$$
\begin{align*}

\nabla_\theta J(\theta) &= \sum^{T-1}_{t=0} \mathbb{E}_{\tau \sim \pi_\theta}\left[\gamma^t G_t \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] \\

\theta_{t+1} &= \theta_{t} + \eta \gamma^t G_t \nabla_\theta \ln \pi_\theta(a_t|s_t) \\

\end{align*}
$$

Though, by introducing the discount factor, we have also brought one obvious caveat to home: our target is no more a true return but discounted return!

Let's be dramatic and define a step function-like discount factor sequence:

$$

\gamma_t = 

\begin{cases}
1 && \text{if } t \le k \\
0 && \text{otherwise}
\end{cases}

$$

Then, our discounted return $G_0 = \sum^{k}_{t=0} R_{t+1}$, which is not a true return when $k < T$.

So, don't forget - although the discount factor is common and powerful tool in RL, it introduces bias if the original target is undiscounted one.

</details>

<details>
    <summary> (Optional) Policy Gradient Theorem </summary>

Being a careful reader, you may have noticed that I was being a bit sloppy in the derivation. Like, can we define the "goodness" of the policy and give them an order? So that we can say "we successfully improved the policy"?

Well, as I apologized ealier, this seems to be where the value function $v_{\pi}(s)$ and action-value function $q_{\pi}(s,a)$ gets pretty handy. Trying to headbutt into finding the best policy, we derived things without explicitly relying on the value functions. Though, having the concept of value function would make our life much easier. How ironic!

That's why this section is optional and long. If you are not very interested / comfortable with the value function, you can come back to this section later after we cover some early value-based methods.

---

The Policy Gradient Theorem is a powerful tool that connects policy gradient to the value function. The gradient of the return, or the objective function $J(\theta)$ we defined earlier, can be concisely expressed under the value function.

Before diving into the theorem, let's shortly define these:

$$
\begin{align*}
v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t|S_t = s] && \text{Value function} \\
&= \mathbb{E}_{\pi}[R_{t+1} + R_{t+2} + \ldots | S_t = s] && \text{Undiscounted case} \\
q_{\pi}(s,a) &= \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] && \text{Action-value function} \\
&= \mathbb{E}_{\pi}[R_{t+1} + R_{t+2} + \ldots | S_t = s, A_t = a] \\
\end{align*}
$$

Here, let's assume we know both $v_{\pi}(s)$ and $q_{\pi}(s,a)$ for the current policy $\pi$. Learning these functions is a totally different story, and we will cover them later.

Also, don't confuse them with the true optimal value functions $v^*(s)$ and $q^*(s,a)$, which are the best value functions that can be achieved in the environment. True optimal functions are policy-independent, while the functions we defined are policy-dependent.

Knowing these, we can compare different policies $\pi$ versus $\pi'$. Here, you may ask a question: isn't the policy that maximizes the expected total return as we defined earlier the best policy? In multi-armed bandit or simple gridworld environment, yes, this is true! However, this is because we implicitly assumed the initial state is fixed.

Let's assume we have two initial states $s_A$ and $s_B$. At the beginning of every episode, the agent is  placed in either $s_A$ or $s_B$. Then, given two policies $\pi_A$ and $\pi_B$, a deadlock can occur: $G_{\pi_A}(s_A) > G_{\pi_B}(s_A)$ but at the same time $G_{\pi_A}(s_B) < G_{\pi_B}(s_B)$. In this case, it is a bit hard to say which policy is better.

This shows us that defining the total order of the policy set is not always possible. Still, we can define a partial order on the same set. This is where the value function comes in handy: for every state $s \in S$, we define partial order as:

$$
\begin{align*}

v_{\pi}(s) &\geq v_{\pi'}(s) && \text{for all } s \in S \\

\end{align*}
$$

...where $\pi$ and $\pi'$ are two different policies. If we can satisfy this inequality given two policies, we can say $\pi$ is better than $\pi'$.


---


We defined a partial order of the policy. Let's leverage this to derive a Policy Gradient Theorem. A useful primer for the derivation:
$$
\begin{align*}

v_{\pi}(s) &= \sum_a \pi(a|s)q_{\pi}(s,a) && \text{Marginalized action} \\
q_{\pi}(s,a) &= \mathbb{E}_\pi[R_{t+1} + G_{t+1}|S_t = s, A_t = a]  \\
&= \sum_{s'}p(s'|s,a) \mathbb{E}_\pi[R_{t+1}|S_t = s, A_t = a, S_{t+1} = s'] && \text{Immediate reward} \\

&\qquad + \sum_{s'}p(s'|s,a) \mathbb{E}_\pi[G_{t+1}|S_t = s, A_t = a, S_{t+1} = s'] && \text{Next state value} \\

&= \sum_{s'}p(s'|s,a) r(s,a,s') && \text{Reward function} \\
&\qquad + \sum_{s'}p(s'|s,a) v_{\pi}(s') && \text{Markov property} \\

&= \sum_{s'}p(s'|s,a) [r(s,a,s') + v_{\pi}(s')] \\

&= \sum_{s',r}p(s',r|s,a) [r + v_{\pi}(s')] && \text{Sutton \& Barto's notation} \\

\end{align*}
$$

Now, we are ready to state the Policy Gradient Theorem (for the episodic case). You can also find this, and continuous case, in the Sutton & Barto's book.

$$
\begin{align*}

\nabla_\theta v_{\pi}(s) &= \nabla_\theta \mathbb{E}_{\pi}\left[\sum_a \pi(a|s)q_{\pi}(s,a)\right] \\

&= \sum_a \mathbb{E}_{\pi}\left[\nabla_\theta \pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla_\theta q_{\pi}(s,a)\right] \\
&= \sum_a \mathbb{E}_{\pi}\left[\nabla_\theta \pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla_\theta \sum_{s'}p(s'|s,a) [r(s,a,s') + v_{\pi}(s')]  \right] \\

&= \sum_a \mathbb{E}_{\pi}\left[\nabla_\theta \pi(a|s)q_{\pi}(s,a) + \pi(a|s)\sum_{s'}p(s'|s,a) \nabla_\theta v_{\pi}(s')  \right] \\

\end{align*}
$$

Oops! We found a recursive structure - we found $v_{\pi}(s')$ at the end! Unrolling this, we can reach to a neat expression. Allow me to abuse notation a bit here:

$$
\begin{align*}

\nabla v_s &= \sum_a (\nabla \pi_{as}q_{as} + \pi_{as}\sum_{s'}p_{sas'}\nabla v_{s'}) \\

&= \sum_a \nabla \pi_{as}q_{as} + \sum_a \pi_{as}\sum_{s'}p_{sas'}(\sum_{a'}\nabla \pi_{a's'}q_{a's'} + \pi_{a's'}\sum_{s''}p_{s'a's''}\nabla v_{s''}) \\

&= \sum_a \nabla \pi_{as}q_{as} + \sum_a \pi_{as}\sum_{s'}p_{sas'}\sum_{a'}\nabla \pi_{a's'}q_{a's'} \\
&\qquad + \sum_a \pi_{as}\sum_{s'}p_{sas'}\pi_{a's'}\sum_{s''}p_{s'a's''}\nabla v_{s''} \\

&= \ldots \\

&= \sum_{x\in S}\left[I(s,x) + \sum_a\pi_{as}p_{sax} +  \sum_a\pi_{as}\sum_{s'}p_{sas'}\sum_{a'}\pi_{a's'}p_{s'a'x}+\ldots\right]\sum_a \nabla \pi_{ax}q_{ax} \\

\end{align*}
$$

Therefore, we can reach to this general form for any state $s \in S$:

$$
\begin{align*}

\nabla_\theta v_{\pi}(s) &= \sum_{x\in S}\sum_{k=0}^{\infty} P(s\to x,k,\pi)\sum_a \nabla_\theta \pi(a|x)q_{\pi}(x,a) \\

\end{align*}
$$

...where $P(s\to x,k,\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\pi$.

---

Okay, now we have the gradient of the value function for any given state $s \in S$. We are theoretically ready - so let's see the simplest case to get some tangible sense.

Let's assume an environment where you deterministically initialize at state $s_0 \in S$. Then, the definition of the gain, value function, and our objective function boils down to:

$$
\begin{align*}

J(\theta) &= v_{\pi}(s_0) \\

\end{align*}
$$

Then, let's find a better language to express our predefined function $P(s\to x,k,\pi)$. By defining the so-called on-policy distribution:

$$
\begin{align*}

h(s) &\triangleq p(s_0 = s) && \text{Initial state distribution} \\
\rho(s) &= h(s) + \sum_{s_{-1}}\rho(s_{-1})\sum_{a}p(s|s_{-1},a)\pi(a|s_{-1}) && s_{-1} = \text{previous states} \\
\mu(s) &= \frac{\rho(s)}{\sum_{x}\rho(x)} && \text{Normalized on-policy distribution} \\

\end{align*}
$$

By using the on-policy distribution, we can substitute the probability function in our gradient expression:

$$
\begin{align*}

\nabla_\theta J(\theta) &= \nabla_\theta v_{\pi}(s_0) \\

&= \sum_s \sum_{k=0}^{\infty} P(s_0\to s,k,\pi)\sum_a \nabla_\theta \pi(a|s)q_{\pi}(s,a) \\

&= \sum_s \rho(s)\sum_a \nabla_\theta \pi(a|s)q_{\pi}(s,a) && \text{On-policy distribution} \\
&\propto \sum_s \mu(s)\sum_a \nabla_\theta \pi(a|s)q_{\pi}(s,a) && \text{Normalize} \\

\end{align*}
$$

This derivation plus Monte Carlo sampling beautifully leads to the REINFORCE algorithm we just discussed without value function. Again,

$$
\begin{align*}

\nabla_\theta J(\theta) &\propto \sum_s \mu(s)\sum_a \nabla_\theta \pi(a|s)q_{\pi}(s,a) \\

&= \mathbb{E}_{S_t\sim\pi}\left[\sum_a \nabla_\theta \pi(a|S_t)q_{\pi}(S_t,a)\right] && \text{Sample state} \\

&= \mathbb{E}_{S_t\sim\pi}\left[\sum_a\pi(a|S_t)\nabla_\theta \ln \pi(a|S_t)q_{\pi}(S_t,a)\right] && \text{Log derivative trick} \\

&= \mathbb{E}_{(S_t, A_t)\sim\pi}\left[\nabla_\theta \ln \pi(A_t|S_t)q_{\pi}(S_t,A_t)\right] && \text{Sample action} \\

&= \mathbb{E}_\pi\left[G_t \nabla_\theta \ln \pi(A_t|S_t)\right] && \text{REINFORCE} \\

\end{align*}
$$


</details>

<details>
    <summary> Continuous case? </summary>

Generalizing the discrete case to the continuous case is straightforward, so we will not cover it in detail. Replace the sum in our steps with the integral, careful with what variable you are integrating over, and you are good to go.

</details>

<br>
<br>

# 2. Baseline Subtraction and Node Perturbation

In the last section, we mentioned REINFORCE is one of the Monte Carlo methods. The pros of REINFORCE is its unbiasedness in estimating the policy gradient: given enough samples, the estimated gain will converge to the true target gain. However, it has also been widely known that REINFORCE has a high variance and slow convergence rate. This problem arises as we have only focused on "correctly" (or, unbiasedly) estimating the gradient but not on the variance of the estimation. Moreover, the return $G_t$ can only be correctly calculated at the end of the episode, which makes the learning process slow and sometimes even infeasible.

To solve the former, the variance issue, many textbooks and practical applications of the vanilla REINFORCE algorithm suggest baseline subtraction as a simple yet effective approach to reduce the variance of the policy gradient. A common formulation follows:

$$
\begin{align*}

\nabla_\theta J(\theta) &= \sum_t\mathbb{E}_{\tau \sim \pi_\theta}\left[G_t \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] \\

&= \sum_t\mathbb{E}_{\tau \sim \pi_\theta}\left[(G_t - b(s_t)) \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] + \sum_t\mathbb{E}_{\tau \sim \pi_\theta}\left[b(s_t) \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] && \text{Add and subtract } b(s) \\

&= \sum_t\mathbb{E}_{\tau \sim \pi_\theta}\left[(G_t - b(s_t)) \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] + \sum_t\mathbb{E}_{s\sim\pi_\theta}[b(s_t) \nabla_\theta 1] && \text{Expectation of score function} \\

&= \sum_t\mathbb{E}_{\tau \sim \pi_\theta}\left[(G_t - b(s_t)) \nabla_\theta \ln \pi_\theta(a_t|s_t)\right] && \text{Constant gradient} \\

\end{align*}
$$

Therefore, introducing action-independent baseline $b(s)$ (or, even a state-independent baseline $b$) does not change the expectation of the policy gradient.

Then, why does baseline subtraction reduce the variance of the policy gradient? Well, the straightforward approach here is to calculate the variance of the policy gradient. Though, I believe comparing REINFORCE to the node perturbation method will give you a more intuitive understanding here.

---

More detailed explanation of the node perturbation method can be found [here](../Learning%20Rule/1.%20Perturbation.md#2-node-perturbation), but no need to worry if you are not familiar with it. We will only focus on the comparison between the two methods.

---

For this intuitive understanding, let's assume a feedforward network approximating the policy $\pi_\theta$. We have a 1-dimensional continuous action space. We can specifically focus on the very last layer of the network. We can write this as:

$$
\begin{align*}

\bar{a_t} &= Wx_t && \text{dropped bias for simplicity} \\
a_t &= \bar{a_t} + \sigma\xi_t && \text{Stochastic policy}

\end{align*}
$$

...where $W$ is the weight matrix of the output layer, $x_t$ is the input to the output layer, $\sigma$ is the standard deviation of the noise, and $\xi_t$ is the Gaussian white noise that satisfies $\mathbb{E}[\xi_t \xi_t^T] = I$.

In this setting, we can say our stochastic policy $\pi_\theta$ performs a Gaussian sampling. Further, we can see we swapped the deterministic output unit of the neural network with a Gaussian stochastic unit with mean $\bar{a_t}$ and standard deviation $\sigma$.

Then, we can compute $\nabla_{\bar{a_t}} \ln \pi_\theta$:

$$
\begin{align*}

\pi_\theta(a_t|x_t) &= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(a_t - \bar{a_t})^2}{2\sigma^2}\right) \\

\ln \pi_\theta(a_t|x_t) &= -\frac{1}{2}\ln(2\pi\sigma^2) - \frac{(a_t - \bar{a_t})^2}{2\sigma^2} \\

\nabla_{\bar{a_t}} \ln \pi_\theta(a_t|x_t) &= \frac{a_t - \bar{a_t}}{\sigma^2} \\
&= \frac{\xi_t}{\sigma}

\end{align*}
$$

Then, by using the chain rule:

$$
\begin{align*}

\nabla_{\theta} \ln \pi_\theta(a_t|x_t) &= \nabla_{\bar{a_t}} \ln \pi_\theta(a_t|x_t) \nabla_{\theta} \bar{a_t} \\
&= \frac{\xi_t}{\sigma} x_t^T

\end{align*}
$$

Plugging this back to the policy gradient:

$$
\begin{align*}

\eta G_t \nabla_{\theta} \ln \pi_\theta(a_t|x_t) &= \frac{\eta}{\sigma} G_t \xi_t x_t^T && \text{REINFORCE} \\
&= \frac{\eta}{\sigma} (G_t - 0) \xi_t x_t^T \\
&= \frac{\eta}{\sigma} (\mathcal{L}(\bar{a_t} + \sigma\xi_t) - \mathcal{L}(\bar{a_t})) \xi_t x_t^T && \text{Node perturbation form} \\

\end{align*}
$$

Then this gets us a better intuition on why the baseline subtraction helps: the REINFORCE without baseline subtraction is equivalent to the node perturbation method where clean path $\mathcal{L}(\bar{a_t})$ achieves zero return!

From this perspective, we can see the baseline subtraction as a "deterministic" expected policy path. By choosing a proper baseline $b(s)$ that approximates $\mathcal{L}(\bar{a_t})$ better than 0, we can reduce the variance of the policy gradient.

<details>
    <summary> Variance calculation and Value function baseline </summary>

We can make an attempt to calculate the variance of the policy gradient. Given the fact that the baseline subtraction does not change the expectation of the policy gradient, we would love to minimize $\mathbb{E}(X^2)$ term to minimize variance (as $Var[X] = \mathbb{E}(X^2) - \mathbb{E}(X)^2$).

Also, I might be very wrong at this derivation - please feel free to correct me. Anyway, my attempt with the Gaussian stochastic policy:

$$
\begin{align*}

\mathbb{E}_{\tau \sim \pi_\theta}[X^2] &= \mathbb{E}_{\tau \sim \pi_\theta}\left[\left(\sum_t(G_t - b(s_t)) \xi_t x_t^T\right)^2\right] && \text{Node perturbation form} \\

&= \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t(G_t - b(s_t))^2 \xi_t x_t^T x_t \xi_t^T\right] \\
&\qquad + \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_i\sum_{j\neq i}(G_i - b(s_i))(G_j - b(s_j)) \xi_i x_i^T x_j \xi_j^T\right] \\

&= \sum_t\mathbb{E}_{\tau \sim \pi_\theta}\left[(G_t - b(s_t))^2 x_t^T x_t\right] + 0 && \mathbb{E}[\xi_i\xi_{j\neq i}] = 0, x_i^T x_j = \text{scalar} \\


\end{align*}
$$

Then, if my attempt is correct, we can see the variance of the policy gradient is minimized when the baseline $b(s_t)$ is equal to the expected gain $\mathbb{E}[G_t|s_t]$ - which is, by definition, the value function $v_\pi(s)$! Indeed, the value function $v_{\pi}(s)$ is a common choice for the baseline $b(s)$.

For multidimensional action space, our sampled noise $\xi$ would be a vector rather than a scalar as we assumed in this simple case. This would make the variance calculation more complicated (e.g. we may have to use the Frobenius norm rather than the $\xi x^T x \xi^T$ term), but I believe the intuition remains the same.

</details>

<br>
<br>

# 3. Closing
In this section, we have covered the basics of the policy gradient method. We have seen how the REINFORCE algorithm can be derived from the straightforward objective of maximizing the expected return. We have also seen the connection between the policy gradient and the node perturbation method. We extended this to get an intuitive understanding of why baseline subtraction can reduce the variance of the policy gradient.

However, we left some crumbs behind - we didn't touch the second cons of the REINFORCE algorithm, the $G_t$ calculation at the end of the episode. This is a general issue for any Monte Carlo method. How can we mitigate this issue?

Turns out, we can sacrifice the unbiasedness of the policy gradient estimation to get a more efficient learning process. This leads to the general actor-critic framework, where we use a value function to estimate and replace the return $G_t$ on-the-fly. We will cover this in the next section.