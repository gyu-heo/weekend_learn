# 0. Gradient Descent

We will not discuss any details about the gradient descent here. Let's just have a look at the basic idea and formulation.

Given a function $f(W,x): \mathbb{R}^n \to \mathbb{R}$ with parameters $W$, we want to find the optimal $W^*$ that minimizes the predefined loss function $ \mathcal{L}(y, f(W,x))$.

Intuitively, you can measure how much the loss function changes as you nudge the parameters $W$ a little bit. If you can find the direction in which the loss function decreases the steepest, you may wanna move the parameters in that direction, in a hope to minimize the loss function. This is the basic idea of gradient descent.

This direction can be formally written as $\nabla_W \mathcal{L}$, which is the gradient of the loss function with respect to the parameters $W$.

---

Now, assume that your function $f(W,x)$ is MLP (multilayer perceptron) with $W = \{W_1, W_2, \ldots, W_L\}$, where $L$ is the number of layers in the network.

The gradient of the loss function with respect to the parameters $W$ can be computed using the chain rule of calculus as:

$$
\nabla_W \mathcal{L} = \left[ \frac{\partial \mathcal{L}}{\partial W_1}, \frac{\partial \mathcal{L}}{\partial W_2}, ..., \frac{\partial \mathcal{L}}{\partial W_L} \right]
$$

---

For MLP, let's say each layer k is defined as:

$$
\begin{align*}
x_k = \sigma_k(W_k x_{k-1}) && \text{for } k = 1, 2, ..., L
\end{align*}
$$

where $x_0 = x$ is the input, and $x_L = f(W,x)$ is the output of the network. $\sigma_k$ is the activation function at layer k.

By the chain rule, the gradient of the loss function $\mathcal{L}$ with respect to the parameters $W_k$ can be computed as:

$$
\begin{align*}
\frac{\partial \mathcal{L}}{\partial W_k} &= \frac{\partial \mathcal{L}}{\partial x_L} \frac{\partial x_L}{\partial x_{L-1}} \frac{\partial x_{L-1}}{\partial x_{L-2}} ... \frac{\partial x_{k+1}}{\partial x_k} \frac{\partial x_k}{\partial W_k} \\

&= \frac{\partial \mathcal{L}}{\partial x_L} W_L^T W_{L-1}^T ... W_{k+1}^T \frac{\partial x_k}{\partial W_k} && \text{for linear activation function} \\
\end{align*}
$$

<details>
    <summary> BPTT? </summary>

Backpropagation through time (BPTT) is a variant of the backpropagation algorithm for RNN. You can see this as an weird variant of the MLP with the unfolded sequence length T.

Let's say we have a simple RNN with many-to-many sequence tasks. For each time step t, the RNN generates the output $y_t$ and receives the loss feedback $\mathcal{L}_t$.

Then, we would love to minimize the total loss $\mathcal{L} = \sum_{t=1}^T \mathcal{L}_t$. The gradient of the total loss with respect to the weights can be computed as:

$$
\begin{align*}

h_t &= f(x_t, h_{t-1}, w_h) && w_h \text{ as hidden weight} \\
y_t &= g(h_t, w_y) && \text{simple RNN} \\

\frac{\partial \mathcal{L}}{\partial w_h} &= \sum_{t=1}^T \frac{\partial \mathcal{L}_t}{\partial w_h} \\

&= \sum_{t=1}^T \frac{\partial \mathcal{L}_t}{\partial y_t} \frac{\partial y_t}{\partial h_t} \frac{\partial h_t}{\partial w_h} \\


\end{align*}
$$

Now, the first two terms are straightforward. The last term $\frac{\partial h_t}{\partial w_h}$ is the tricky part, as it involves the past hidden states. Simply, you can see this one-step chain rule as:

$$
\begin{align*}

\frac{\partial h_t}{\partial w_h} &= 
\underbrace{\frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h}}_{\text{Direct effect}}
+ 
\underbrace{\frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}}_{\text{Indirect effect via } h_{t-1}}.
\end{align*}
$$

Here we found a recursive relation in gradients over time steps as $a_t = b_t + c_t a_{t-1}$, which can be unfolded to the full sequence length T as $a_T = b_T + \sum_{t=1}^{T-1} \left( \prod_{k=t+1}^T c_k \right) b_t$ (given $a_0 = 0$).

Plug this back to the gradient calculation, we can get the BPTT algorithm. Let's calculate $\frac{\partial \mathcal{L}_T}{\partial w_h}$ as an example:

$$
\begin{align*}

\frac{\partial \mathcal{L}_T}{\partial w_h} &= \frac{\partial \mathcal{L}_T}{\partial y_T} \frac{\partial y_T}{\partial h_T} \frac{\partial h_T}{\partial w_h} \\

&= \frac{\partial \mathcal{L}_T}{\partial y_T} \frac{\partial y_T}{\partial h_T} \left( \frac{\partial f(x_T, h_{T-1}, w_h)}{\partial w_h} + \sum_{t=1}^{T-1} \left( \prod_{k=t+1}^T \frac{\partial f(x_k, h_{k-1}, w_h)}{\partial h_{k-1}} \right) \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} \right)

\end{align*}
$$


</details>

---

However, we believe the backpropagation algorithm is not biologically plausible. The main problem is each $W_k^T$ in the above equation, which requires the global information of the network.
<details>
    <summary> Weight Transport Problem... </summary>

This problem is known as the weight transport problem. blah blah blah...

</details>

<br>
<br>

# 1. Weight Perturbation

For the dimensionality notation, let's assume each hidden layer has the same number of neurons $H$. Input $x$ is a vector of size $D$, and output $y$ is a vector of size $O$.

---

If we cannot compute $\frac{\partial \mathcal{L}}{\partial W_k}$, how can we update the weights $W_k$? One intuition is that, although you may not know the optimal direction to update the weights, you can still try to nudge the weights a little bit and see how the loss function changes. In other words, you can sample the direction to update the weights. If your gradient estimator is unbiased, then the sample mean of the direction will converge to the true gradient.

Then, which gradient estimator would you want to build? Following the intuition above, you can sample the direction to update the weights by adding a small random noise to the weights. This is the basic idea of weight perturbation.

For a 1 layer linear network with a scalar target, let's say we have a small Gaussian noise $\sigma \xi$ where $\mathbb{E}[\xi\xi^T] = I_D$ and $\sigma \ll 1$. Now, inject this noise to perturb the weights $W \in \mathbb{R}^{D \times 1}$. Then, all we need to know is how the loss function changes by. We can define a scalar gain $G \triangleq \mathcal{L}(\sigma \xi) - \mathcal{L}(0)$. Here, $\mathcal{L}(0)$ is the loss without any noise perturbation.

How good is this gain $G$ as an estimator of the gradient? Let's compute the first-order Taylor expansion of the loss function $\mathcal{L}$ around the origin:

$$
\begin{align*}
G &= \mathcal{L}(\sigma \xi) - \mathcal{L}(0) \\

&\approx (\mathcal{L}(0) + \sigma \nabla_W \mathcal{L}(0) \xi) - \mathcal{L}(0) && \text{first-order Taylor expansion} \\

&= \sigma \nabla_W \mathcal{L}(0) \xi && \text{with gradient } \nabla_W \mathcal{L} \in \mathbb{R}^{1 \times D} \\
\end{align*}
$$

Hooray! We recovered $\nabla_W \mathcal{L}$ from the gain $G$! By leveraging the feature of the Gaussian noise mentioned above, $\mathbb{E}[\xi\xi^T] = I_D$, we can further show the update direction $G\xi$:

$$
\begin{align*}
\mathbb{E}[G \xi] &\approx \mathbb{E}[(\sigma \nabla_W \mathcal{L}(0) \xi) \xi] \\

&= \mathbb{E}[\xi (\xi^T \sigma \nabla_W \mathcal{L}(0)^T)] && \text{Transpose scalar}\\

&= \sigma \mathbb{E}[\xi \xi^T] \nabla_W \mathcal{L}(0)^T && \text{Expectation over the noise} \\

&= \sigma \nabla_W \mathcal{L}(0)^T && \text{Transposed only to match the dimension} \\

\end{align*}
$$

By dividing each side by $\sigma$, we can get the update direction of the weight perturbation as an unbiased estimator of the true gradient.

Following this, we can update weights with K noise samples:
$$
\begin{align*}
\Delta W &= -\frac{\eta}{\sigma} \frac{1}{K} \sum_{k=1}^K G^{(k)} \xi^{(k)} \\
\Delta W_i &= -\frac{\eta}{\sigma} \frac{1}{K} \sum_{k=1}^K G^{(k)} \xi_i^{(k)}
\end{align*}
$$

<details>
    <summary> What if the target dimension is not a scalar? </summary>

Surely, for the non-scalar target, we can flatten $W_{ij}$ and $\xi_{ij}$ to the vector form. Then, the update calculation should be the same as above.


Note that, the loss function $\mathcal{L}$ should still be a scalar function.

</details>

<details>
    <summary> Multilayer network? </summary>

The update calculation for the multilayer network is the same as above. For L-layer MLP, we can again flatten the weights of each layer to create a pooled weight vector $W \in \mathbb{R}^{D*H + (L-2)*H^2 + H*O}$. Then, we can sample the noise $\xi$ independently and proceed with the same update calculation.

Note that all your synapse remembers, given the correct scalar difference $G$, is the noise it received $\xi_{ij}^l$ (in its own layer).

</details>

<details>
    <summary> Extention to RNN? </summary>

Applying weight perturbation to RNN for sequence length T with the loss function at the end of the sequence (i.e. delayed reward) could be a bit confusing. The most straightforward approach is to assume the static noise over the sequence. Then, we can still calculate $\mathcal{L}(\sigma \xi) - \mathcal{L}(0)$ and move on with the update calculation. [This is one of the recent reference assuming the static noise.](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.13.021006)

The issue gets nuanced when the noise is dynamic. If you have a dense loss feedback at each time step, you can still apply the weight perturbation as above. Simply iterate: assume the "noiseless" network at time step T is the same as the network with noise at the previous time step.

However, if you have a delayed reward, you may need to consider the temporal correlation of the noise. Intuitively, your RNN becomes close to T-layered MLP with weight sharing layers, but different noise injected at each layer. I am not very sure about the exact update calculation in this case. I may need inputs from learning rule gurus.

Well, anyway, my intuition of the weight perturbation stops with this triad of:

1. Dynamic noise over the sequence (fix: static noise)
2. Sparse loss feedback (fix: dense loss feedback)
3. Weight sharing over the sequence

Would love to hear more about the update calculation in this case!
</details>

<br>
<br>

# 2. Node Perturbation

The weight perturbation is a simple and intuitive way to estimate the gradient. However, it has a limitation that it requires a large number of samples to estimate the gradient accurately. Applying WP to the recurrent network in a specific task environment can be even more challenging. This is because the noise is injected to the weights.

We can mitigate this problem by injecting the noise to the activations (nodes) instead of the weights. This is the basic idea of node perturbation.

Again, let's start with the 1-layer, scalar target network. To avoid potential confusion coming from perturbing the activity, not weight, let's assume the layer has an arbitrary activation function $f$. Then, we can again inject a small Gaussian noise $\sigma \xi$ as we did in the weight perturbation:

$$
\begin{align*}
r &= W\mathbf{x} && \text{noiseless pre-activation} \\
y(0) &= f(r) && \text{noiseless output} \\
y(\sigma \xi) &= f(r + \sigma \xi) && \text{noisy output}
\end{align*}
$$

where again, here $\mathbf{x}$ is $D$ dimensional input and $y$ is a scalar output. We assume the same noise property $\mathbb{E}[\xi\xi^T] = I_D$ and $\sigma \ll 1$.

Then, the idea is same: we would love to 1) know how the loss function changes by the noise perturbation and 2) estimate the gradient from the change. Allow me to abuse the notation and use $G$ as $\mathcal{L}(\sigma \xi) - \mathcal{L}(0)$. In the previous section this meant the gain from the weight perturbation, but here it means the gain from the node perturbation.

Then, we can proceed as before:

$$
\begin{align*}

G &= \mathcal{L}(\sigma \xi) - \mathcal{L}(0) \\
&\approx (\mathcal{L}(0) + \sigma \nabla_r \mathcal{L}(0) \xi) - \mathcal{L}(0) \\
&= \sigma \nabla_r \mathcal{L}(0) \xi


\end{align*}
$$

Following this,

$$
\begin{align*}

\mathbb{E} \left[ \frac{G \xi}{\sigma} \right] &\approx \nabla_r \mathcal{L}(0)^T && \text{(again, we use row vector notation here)}

\end{align*}
$$

Note that the gradient $\nabla_r \mathcal{L}$ is the gradient of the loss function with respect to the output $r$. This is the main difference from the weight perturbation.

Obviously, in our setup, chain rule can reconstruct the gradient $\nabla_W \mathcal{L}$:

$$
\begin{align*}

\frac{\partial \mathcal{L}}{\partial W} &= \frac{\partial \mathcal{L}}{\partial r} \frac{\partial r}{\partial W} \\
&= \nabla_r \mathcal{L} \, \mathbf{x}^T

\end{align*}
$$

Then, we can compute the update direction of the node perturbation with K noise samples as:

$$
\begin{align*}

\Delta W &= -\frac{\eta}{\sigma} \frac{1}{K} \sum_{k=1}^K G^{(k)} \xi^{(k)} \, \mathbf{x}^T \\
\Delta W_i &= -\frac{\eta}{\sigma} \frac{1}{K} \sum_{k=1}^K G^{(k)} \xi_i^{(k)} \, \mathbf{x}_i \\

\end{align*}
$$


<details>
    <summary> Multilayer network? </summary>

Theoretically, we can move on with the similar flattening approach we did in the weight perturbation: only flatten the pre-activations, not the weights. However, I noticed this is not how the node perturbation is usually implemented in the literature. Many authors tend to approach layerwise. Here, we'll follow the common notation (which is basically the same thing).

For the L-layer MLP, let's say $\Xi = \{\xi^{(1)}, \xi^{(2)}, ..., \xi^{(L)}\}$ is the set of noise samples injected to each layer. Activation of each layer l is defined as $\mathbf{x}^{(l)} = f_k(W^{(l)} \mathbf{x}^{(l-1)})$.

Then, we can proceed with the multivariate First-order expansion around clean path $r$ by summing the gradients of each layer:

$$
\begin{align*}

\mathcal{L}(\Xi) & \approx \mathcal{L}(0) + \sum_{l=1}^L \sigma \nabla_{\mathbf{r}^{(l)}} \mathcal{L}(0) \xi^{(l)} \\

\end{align*}
$$

Thanks to the property of the noise we chose, the expectation for layer $l$ immediately follows:
$$
\begin{align*}
\mathbb{E} \left[ \frac{G \xi^{(l)}}{\sigma} \right] &\approx \nabla_{\mathbf{r}^{(l)}} \mathcal{L}(0)^T && (\because \mathbb{E}[\xi^{(l)}\xi^{(m \neq l)}] = \mathbb{0})
\end{align*}
$$

...and this is the partial gradient of the loss function w.r.t layer $l$ pre-activation, considering all the layers above it. As we did in 1-layer case, we can proceed with the chain rule and get the update direction of the node perturbation:

$$
\begin{align*}

\Delta W^{(l)} &= -\frac{\eta}{\sigma} G \xi^{(l)} \, (\mathbf{x}^{(l-1)})^T \\

\end{align*}
$$

(which ended up similar to "flattening" the pre-activations). Note that for the clarity I dropped K noise sampling here, but to leverage the noise property and unbiasedness of the estimator you should still sample the noise K times.

</details>

<details>
    <summary> Extention to RNN? </summary>

Extend the node perturbation to RNN is more straightforward than the weight perturbation. The main reason is that the noise is injected to the activations, not the weights. The update calculation is essentially the same as the multilayer network: for T sequence length, you can sample the noise $\Xi = \{\xi^{(1)}, \xi^{(2)}, ..., \xi^{(T)}\}$ and proceed with the update calculation. We take the multivariate First-order expansion, but now across the sequence (time) dimension.

For the simple RNN, this falls down to the simple update calculation as:

$$
\begin{align*}

h_t &= f(W_h h_{t-1} + W_x x_t) \\
y_t &= W_y h_t && \text{clean RNN path} \\
\\
\tilde{h}_t &= f(W_h h_{t-1} + W_x x_t + \sigma \xi_t) \\
\tilde{y}_t &= W_y \tilde{h}_t + \psi_t && \text{noisy RNN path} \\
\\
G &= \mathcal{L(\tilde{y}_T) - \mathcal{L(y_T)}} && \text{abused notation}\\
\Delta W_h &= -\frac{\eta}{\sigma} G \sum_{t=1}^T \xi_t \, h_{t-1}^T \\
\Delta W_x &= -\frac{\eta}{\sigma} G \sum_{t=1}^T \xi_t \, x_t^T \\
\Delta W_y &= -\frac{\eta}{\sigma} G \sum_{t=1}^T \psi_t \, h_t^T \\

\end{align*}
$$

</details>


<details>
    <summary> REINFORCE algorithm? </summary>

Node perturbation in the commonly used neural network can be seen as "re-stochasticizing" the network: we switch deterministic units to Gaussian units whose mean is the deterministic output and whose variance is the noise level. This is interesting as it can be seen as a probability distribution over unit's output.

Let's assume the deterministic output of ith unit in layer l is $\mu = Wx^{(l-1)}$, and the sampled noise variance is $\sigma^2$. Then, the output distribution follows:

$$
p(x_i^{(l)}|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i^{(l)}-\mu)^2}{2\sigma^2}\right)
$$

...where $x_i^{(l)}$ denotes the sampled (or, perturbed) output of this unit. Then, we can compute the gradient of the log likelihood of the output w.r.t. $\mu$ and $\sigma$:

$$
\begin{align*}

\frac{\partial \ln p(x_i^{(l)})}{\partial \mu} &= \frac{x_i^{(l)}-\mu}{\sigma^2} \\
\frac{\partial \ln p(x_i^{(l)})}{\partial \sigma} &= \frac{(x_i^{(l)}-\mu)^2 - \sigma^2}{\sigma^3}

\end{align*}
$$

Surely, by chain rule, we can extend this to the gradient w.r.t. the weights $W_{ij}$ as $\frac{x_i^{(l)}-\mu}{\sigma^2}$ times the previous layer activation $x^{(l-1)}_j$. Then, voala! We have the update rule of the node perturbation.

This approach can be seen as a form of the REINFORCE algorithm introduced in [Williams 1992](https://link.springer.com/article/10.1007/BF00992696). As you have already noticed, the node perturbation rule with Gaussian white noise (which we discussed here) is a variant of the REINFORCE algorithm with Gaussian stochastic units, only with the noise level $\sigma$ fixed. This is also noted in the node perturbation paper by [Fiete and Seung 2006](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.97.048104) and also [Miconi 2017](https://elifesciences.org/articles/20899).

</details>