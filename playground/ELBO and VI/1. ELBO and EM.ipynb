{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Downside of VI and ELBO\n",
    "\n",
    "Letâ€™s break this down carefully because there are actually two different layers of â€œparametricâ€ assumptions in Bayesian modeling:\n",
    "\n",
    "Parametric Model Assumption (the Bayesian model):\n",
    "\n",
    "We choose a prior \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "p(z) (e.g., a Gaussian) and a likelihood \n",
    "ğ‘\n",
    "(\n",
    "ğ‘¥\n",
    "âˆ£\n",
    "ğ‘§\n",
    ")\n",
    "p(xâˆ£z) (e.g., another parametric family).\n",
    "This step is common to both MCMC and VI, because it defines the joint \n",
    "ğ‘\n",
    "(\n",
    "ğ‘¥\n",
    ",\n",
    "ğ‘§\n",
    ")\n",
    "p(x,z).\n",
    "In other words, we typically do have a â€œparametricâ€ or at least a specified functional form for \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "p(z) and \n",
    "ğ‘\n",
    "(\n",
    "ğ‘¥\n",
    "âˆ£\n",
    "ğ‘§\n",
    ")\n",
    "p(xâˆ£z). This is what we call the generative model.\n",
    "Inference Method Assumption (approximating the posterior \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ")\n",
    "p(zâˆ£x)):\n",
    "\n",
    "MCMC: Does not require choosing a parametric family for the posterior. Instead, it directly samples from \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ")\n",
    "p(zâˆ£x) (which is fully determined by the model \n",
    "ğ‘\n",
    "(\n",
    "ğ‘¥\n",
    ",\n",
    "ğ‘§\n",
    ")\n",
    "p(x,z)) by building a Markov chain that converges (in principle) to the exact posterior.\n",
    "So MCMC does not parametrically approximate \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ")\n",
    "p(zâˆ£x). It just draws (potentially infinitely many) samples from the true posterior, assuming you run it long enough and the chain mixes well.\n",
    "Variational Inference (VI): Does require choosing a parametric family \n",
    "ğ‘\n",
    "ğœ™\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "q \n",
    "Ï•\n",
    "â€‹\n",
    " (z) to approximate \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ")\n",
    "p(zâˆ£x). For example, a factorized Gaussian, a mixture of Gaussians, or a normalizing flow with parameters \n",
    "ğœ™\n",
    "Ï•. You then optimize \n",
    "ğœ™\n",
    "Ï• (via the ELBO) so that \n",
    "ğ‘\n",
    "ğœ™\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "q \n",
    "Ï•\n",
    "â€‹\n",
    " (z) is as close as possible to the true posterior in some divergence sense.\n",
    "Therefore:\n",
    "\n",
    "Both MCMC and VI share the same â€œparametric modelâ€: they assume you have already picked a prior \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "p(z) and likelihood \n",
    "ğ‘\n",
    "(\n",
    "ğ‘¥\n",
    "âˆ£\n",
    "ğ‘§\n",
    ")\n",
    "p(xâˆ£z). That is a modeling choice.\n",
    "The difference is that MCMC attempts to sample from the true posterior without imposing a further parametric form on \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ")\n",
    "p(zâˆ£x), while VI uses a separate parametric family \n",
    "ğ‘\n",
    "ğœ™\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "q \n",
    "Ï•\n",
    "â€‹\n",
    " (z) to approximate \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ")\n",
    "p(zâˆ£x).\n",
    "Hence:\n",
    "\n",
    "MCMC:\n",
    "Parametric model for prior/likelihood (the usual Bayesian model).\n",
    "But posterior is not forced into a parametric form; you approximate it by sampling from the exact posterior.\n",
    "VI:\n",
    "Same parametric model for prior/likelihood.\n",
    "Additionally picks a parametric family \n",
    "ğ‘\n",
    "ğœ™\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "q \n",
    "Ï•\n",
    "â€‹\n",
    " (z) for the posterior approximation. This can cause â€œvariational biasâ€ if \n",
    "ğ‘\n",
    "ğœ™\n",
    "(\n",
    "ğ‘§\n",
    ")\n",
    "q \n",
    "Ï•\n",
    "â€‹\n",
    " (z) cannot capture all complexities of \n",
    "ğ‘\n",
    "(\n",
    "ğ‘§\n",
    "âˆ£\n",
    "ğ‘¥\n",
    ")\n",
    "p(zâˆ£x).\n",
    "That is the essential distinction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
