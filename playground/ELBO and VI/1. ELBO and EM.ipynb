{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Downside of VI and ELBO\n",
    "\n",
    "Let’s break this down carefully because there are actually two different layers of “parametric” assumptions in Bayesian modeling:\n",
    "\n",
    "Parametric Model Assumption (the Bayesian model):\n",
    "\n",
    "We choose a prior \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "p(z) (e.g., a Gaussian) and a likelihood \n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    "∣\n",
    "𝑧\n",
    ")\n",
    "p(x∣z) (e.g., another parametric family).\n",
    "This step is common to both MCMC and VI, because it defines the joint \n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑧\n",
    ")\n",
    "p(x,z).\n",
    "In other words, we typically do have a “parametric” or at least a specified functional form for \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "p(z) and \n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    "∣\n",
    "𝑧\n",
    ")\n",
    "p(x∣z). This is what we call the generative model.\n",
    "Inference Method Assumption (approximating the posterior \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "p(z∣x)):\n",
    "\n",
    "MCMC: Does not require choosing a parametric family for the posterior. Instead, it directly samples from \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "p(z∣x) (which is fully determined by the model \n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑧\n",
    ")\n",
    "p(x,z)) by building a Markov chain that converges (in principle) to the exact posterior.\n",
    "So MCMC does not parametrically approximate \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "p(z∣x). It just draws (potentially infinitely many) samples from the true posterior, assuming you run it long enough and the chain mixes well.\n",
    "Variational Inference (VI): Does require choosing a parametric family \n",
    "𝑞\n",
    "𝜙\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "q \n",
    "ϕ\n",
    "​\n",
    " (z) to approximate \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "p(z∣x). For example, a factorized Gaussian, a mixture of Gaussians, or a normalizing flow with parameters \n",
    "𝜙\n",
    "ϕ. You then optimize \n",
    "𝜙\n",
    "ϕ (via the ELBO) so that \n",
    "𝑞\n",
    "𝜙\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "q \n",
    "ϕ\n",
    "​\n",
    " (z) is as close as possible to the true posterior in some divergence sense.\n",
    "Therefore:\n",
    "\n",
    "Both MCMC and VI share the same “parametric model”: they assume you have already picked a prior \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "p(z) and likelihood \n",
    "𝑝\n",
    "(\n",
    "𝑥\n",
    "∣\n",
    "𝑧\n",
    ")\n",
    "p(x∣z). That is a modeling choice.\n",
    "The difference is that MCMC attempts to sample from the true posterior without imposing a further parametric form on \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "p(z∣x), while VI uses a separate parametric family \n",
    "𝑞\n",
    "𝜙\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "q \n",
    "ϕ\n",
    "​\n",
    " (z) to approximate \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "p(z∣x).\n",
    "Hence:\n",
    "\n",
    "MCMC:\n",
    "Parametric model for prior/likelihood (the usual Bayesian model).\n",
    "But posterior is not forced into a parametric form; you approximate it by sampling from the exact posterior.\n",
    "VI:\n",
    "Same parametric model for prior/likelihood.\n",
    "Additionally picks a parametric family \n",
    "𝑞\n",
    "𝜙\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "q \n",
    "ϕ\n",
    "​\n",
    " (z) for the posterior approximation. This can cause “variational bias” if \n",
    "𝑞\n",
    "𝜙\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "q \n",
    "ϕ\n",
    "​\n",
    " (z) cannot capture all complexities of \n",
    "𝑝\n",
    "(\n",
    "𝑧\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "p(z∣x).\n",
    "That is the essential distinction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
