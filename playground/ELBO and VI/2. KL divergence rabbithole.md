1. Features of KL divergence / Mutual information
2. Relation to Fisher information, Gauss-Newton, and Hessian
3. Downside of KL divergence: KS divergence, EMD, Wasserstein, ...
4. Optimal transport problem and Sinkhorn algorithm (plus, linear assignment problem as a specific case of OT)

## 1. Basic Features of KL divergence

What would be the intuitive understanding of the KL divergence? Just staring at the equation, it is obvious we have an expectation:

$$
\begin{align*}
D_{KL}(P||Q) &= \int p(x) \ln \left( \frac{p(x)}{q(x)} \right) \, dx \\

&= \mathbb{E}_{p(x)} \ln \frac{p(x)}{q(x)}

\end{align*}
$$

...so, we take an expectation over the subtraction of log probability, $\ln p(x) - \ln q(x)$.

If you are familiar with entropy, information theory, or so, then you may have this nice language: we measure the difference in the information content "$\ln p(x) - \ln q(x)$", and then take expectation over $p(x)$.

In laymen's term (like for me), this is one way to roughly measure overall "error" or "surprisal" when you hypothesized $q(x)$ can approximate $p(x)$. One step further, KL divergence is strict around common frequent events, but less sensitive around rare events.

What do I mean by that? Thanks to the expectation form, your $q(x)$ can be wrong wherever $p(x) \approx 0$. When $p(x) \approx 0$, that event happens rarely. So, even if you messed your approximation around there, it won't surprise you that much.

---

Mutual Information...

---

One basic feature of KL divergence is non-negativity. Again, thanks to the Jensen's inequality,

$$
\begin{align*}

-D_{KL}(P||Q) = \int p(x) \ln \left( \frac{q(x)}{p(x)} \right) \, dx \le \ln \int p(x)\frac{q(x)}{p(x)} \, dx = \ln \int q(x) \, dx = 0

\end{align*}
$$

$$
\begin{align*}

D_{KL}(P||Q) \ge 0

\end{align*}
$$

---

Until this moment, we praised how nice KL divergence is in measuring the difference of two probability distribution. However, it is not a true distance measurement: it is not a metric.

One important note on KL divergence is its asymmetry. Following our flow, $D_{KL}(P||Q)$ is a measurement of surprise when we set $q(x)$ as a hypothesis (or, prior) and use $p(x)$ as a target (or, posterior). $D_{KL}(Q||P)$, on the contrary, is different in its hypothesis. Obviously, this is the condition:

$$
D_{KL}(P||Q) - D_{KL}(Q||P) = \int (p(x)+q(x))\ln  \frac{p(x)}{q(x)} \, dx = 0
$$

This tells us that although KL divergence satisfies the first axiom of metric (non-negativity and identity), it fails on the second (symmetry).

---

As KL divergence is not a metric, it sometimes fails to reflect our intuition on the "difference" between probability distributions.

Let's say we have three Bernoulli distributions:

$$
X(A) = 0.9, X(B) = 0.1 \\
Y(A) = 1.0, Y(B) = 0.0 \\
Z(A) = 0.1, Z(B) = 0.9 \\
$$

...then, which looks closer to which? Surely it depends on your criteria, but in some sense X looks 'closer' to Y than Z. However, $D_{KL}(X||Y)$ explodes to $\infty$, while $D_{KL}(X||Z)$ remains finite.

This unintuitive behavior can also be used as a negative case for the third axiom of metric (triangular inequality).

I wrote a very simple code below. Run this and check the result by yourself.

```python
import torch
from torch.distributions import Bernoulli, kl_divergence

dist_X = Bernoulli(probs=torch.tensor([0.9, 0.1]))
dist_Y = Bernoulli(probs=torch.tensor([1.0, 0.0]))
dist_Z = Bernoulli(probs=torch.tensor([0.1, 0.9]))

KL_XY = kl_divergence(p=dist_X, q=dist_Y)
KL_XZ = kl_divergence(p=dist_X, q=dist_Z)
KL_YZ = kl_divergence(p=dist_Y, q=dist_Z)

KL_YX = kl_divergence(p=dist_Y, q=dist_X)
KL_ZX = kl_divergence(p=dist_Z, q=dist_X)

print(f"X(A) = {dist_X.probs[0]}, X(B) = {dist_X.probs[1]}")
print(f"Y(A) = {dist_Y.probs[0]}, Y(B) = {dist_Y.probs[1]}")
print(f"Z(A) = {dist_Z.probs[0]}, Z(B) = {dist_Z.probs[1]}")

print(f"KL(X||Y) = {KL_XY}")
print(f"KL(X||Z) = {KL_XZ}")

print(f"KL(Y||X) = {KL_YX}")
print(f"KL(Z||X) = {KL_ZX}")

print(f"KL_YZ + KL_XZ - KL_XY = {KL_YZ + KL_XZ - KL_XY}")
```

Now, we all got this very natural question: Do we have a metric between probability distributions? Long story short, surely, yes. However, before jumping into that, let's dig into KL divergence more - and circle back.

<br>
<br>

# 2. Relation to Fisher Information

Further read:

[A Tutorial on Fisher Information](https://arxiv.org/abs/1705.01064)

[Limitations of the Empirical Fisher Approximation for Natural Gradient Descent](https://arxiv.org/abs/1905.12558?ref=inference.vc)

As a dense person, the concept of Fisher information always flew over my head. Like, I heard about Fisher information a few times, but it never clicked in me.

Following Wikipedia, Fisher Information is defined to be the variance of the score function. What is the score function? The score is the gradient of the log likelihood (log probability) function. So,

$$
\begin{align*}
s(\theta) &= \nabla_{\theta} \ln p(x;\theta) && \text{(score function)} \\
I(\theta) &= \text{Var} \left[ s(\theta) \right] && \text{(Fisher Information)} \\

&= \mathbb{E}_{p(x;\theta)} \left[ s^2(\theta) \right] - \mathbb{E}_{p(x;\theta)} \left[ s(\theta) \right]^2 \\

&= \mathbb{E}_{p(x;\theta)} \left[ s^2(\theta) \right] && (\because \mathbb{E}_{p(x;\theta)} \left[ s(\theta) \right] = 0) \\

&= \mathbb{E}_{p(x;\theta)} \left[ \nabla_{\theta} \ln p(x;\theta) \nabla_{\theta} \ln p(x;\theta)^T \right]
\end{align*}
$$

...in other words, the Fisher information is the outer product of the score function, and then expectation.

This is good - but for some reason this didn't click me until at this point.

Here I write down the way how I make sense of Fisher Information by myself: nudging the KL divergence.

---

$p(x;\theta)$ is a vector-to-scalar function with $\mathbb{R}^N \to \mathbb{R}$ where $N$ is the number of parameter $\theta$. Then, we can think of nudging this probability distribution in a parameter space. By simple Taylor expansion around $\theta$ with a small nudging $d\theta$...

$$
\ln p(x;\theta + d\theta) = \ln p(x;\theta) + d\theta^T \nabla_{\theta} \ln p(x;\theta) + \frac{1}{2}d\theta^T \nabla^2_{\theta} \ln p(x;\theta) d\theta + O(||d\theta||^3)
$$

Okay, we already got the hessian, $\nabla^2_{\theta} \ln p(x;\theta)$, in its second-order approximation term! Now, let's substitute this into KL divergence, and measure the difference between two infinitesimally different probability distribution.

$$
\begin{align*}
D_{KL}(p(x;\theta)||p(x;\theta+d\theta)) &= \int p(x;\theta) \left[\ln p(x;\theta) - \ln p(x;\theta+d\theta) \right] \\

&\approx \int p(x;\theta) \left[\ln p(x;\theta) - (\ln p(x;\theta) + d\theta^T \nabla_{\theta} \ln p(x;\theta) + \frac{1}{2}d\theta^T \nabla^2_{\theta} \ln p(x;\theta) d\theta) \right] && \text{Second-order Taylor} \\

&= -\int p(x;\theta) \left[d\theta^T \nabla_{\theta} \ln p(x;\theta) + \frac{1}{2}d\theta^T \nabla^2_{\theta} \ln p(x;\theta) d\theta \right] \\

&= -\int p(x;\theta) \frac{1}{2}d\theta^T \nabla^2_{\theta} \ln p(x;\theta) d\theta && (\because \mathbb{E}_{p}[\nabla_{\theta}\ln p(x;\theta)] = 0) \\

&= -\frac{1}{2}d\theta \,\mathbb{E}_{p(x;\theta)} [\nabla^2_{\theta} \ln p(x;\theta)] \,d\theta^T

\end{align*}
$$

Then, let's briefly pause here and dig into the hessian of the score function:

$$
\begin{align*}

\nabla^2_{\theta} \ln p(x;\theta) &= \nabla_{\theta} \left( \frac{\nabla_{\theta}p(x;\theta)}{p(x;\theta)} \right) \\

&= \frac{\nabla^2_{\theta}p(x;\theta)}{p(x;\theta)} - \frac{\nabla_{\theta}p(x;\theta)\nabla_{\theta}p(x;\theta)^T}{p(x;\theta)^2} \\

&=  \frac{\nabla^2_{\theta}p(x;\theta)}{p(x;\theta)} - \nabla_{\theta} \ln p(x;\theta)\nabla_{\theta} \ln p(x;\theta)^T && \text{Log derivation}

\end{align*}
$$

Then, we plug this back into eq.(5).

$$
\begin{align*}

D_{KL}(p(x;\theta)||p(x;\theta+d\theta)) &\approx -\frac{1}{2} d\theta \, \mathbb{E}_{p(x;\theta)} [\nabla^2_{\theta} \ln p(x;\theta)] \, d\theta^T \\

&= -\frac{1}{2}d\theta \,\mathbb{E}_{p(x;\theta)} \left[ \frac{\nabla^2_{\theta}p(x;\theta)}{p(x;\theta)} - \nabla_{\theta} \ln p(x;\theta)\nabla_{\theta} \ln p(x;\theta)^T \right]\, d\theta^T \\

&= \frac{1}{2}d\theta \,\mathbb{E}_{p(x;\theta)} [\nabla_{\theta} \ln p(x;\theta)\nabla_{\theta} \ln p(x;\theta)^T]\, d\theta^T && (\because \mathbb{E}_{p}[\nabla^2_{\theta}\ln p(x;\theta)] = 0) \\

&= \frac{1}{2} d\theta \,I(\theta)\, d\theta^T \\

( &= \frac{1}{2} d\theta \, \nabla^2_{\theta'} D_{KL}(p_{\theta}||p_{\theta'})|_{\theta'=\theta} \, d\theta^T, \quad \theta'=\theta+d\theta )

\end{align*}
$$

...and this tells us the Fisher Information, or Fisher Information Matrix (FIM), works like hessian here. In other words, the Fisher Information represents how fast the local gradient changes, or, represents local curvatures, of the KL divergence.

Then, you can intuitively see what it means when someone says "large Fisher Information" - the local curvature of KL divergence becomes steeper, with larger variance in gradient. So, the system with larger Fisher Information has a sharp peaks in their parameter space. A.k.a. more confidence in their parameter space.

Be aware that this does not apply to general $D_{KL}(P||Q)$, this only applies when you nudge the distribution. Again, FIM is about the local information.

<details>
    <summary> "Larger" Fisher Information? </summary>
We just mentioned "large Fisher Information" - but how do we measure this with matrix? There seems to be no unique way of ordering in matrix - though, the common choice seems to compare determinant (i.e. $\textit{volume}$ of the Fisher Information), trace (i.e. $\textit{average/sum}$ of it), or whatnot.

</details>

<details>
    <summary> Connection to the second-order optimization? </summary>

If you are familiar with the second-order optimization, you may notice how FIM looks like a special case of the Gauss-Newton matrix. Indeed, if we take the Jacobian $J: \mathbb{R}^{1 \times N}$ in our previous setup (apologies: I usually have column vector notation, but here the Jacobian ended up as a row vector. Sorry for the potential confusion).

Then, obviously, $J^TJ = \mathbb{E}_{p_{\theta}} [\nabla_{\theta} \ln p_{\theta}(x)\nabla_{\theta} \ln p_{\theta}(x)^T] = I(\theta)$.

In conclusion, the Gauss-Newton matrix of KL divergence is the Fisher Information Matrix FIM.

Does it mean we can use Fisher Information as a second-order optimization tool? Indeed, this approach is called Natural Gradient Descent. We will not go over this here, but we'll handle this in Learning Rule directory.

</details>

<details>
    <summary> Pseudocode </summary>

Okay. To get some practical sense, let's play with the Normal distribution. First, we'll sample the gradient and hessian of the log likelihood function. Then, we'll directly compute the hessian of the nudging KL divergence. We'll compare these values!

```python
import torch
from torch.distributions import Normal, kl_divergence
from torch.autograd.functional import jacobian, hessian
import torch.nn.functional as F

torch.set_default_dtype(torch.float64)

## Initialize Normal Distribution
true_mu = 0.0
true_sigma = 1.0
num_samples = 1
num_iters = 1000
nudging_eps = 1e-1

params = torch.tensor([true_mu, true_sigma], requires_grad=True)

## First, sample the gradient and hessian of log-likelihood function
sampled_grad = torch.zeros(2)
sampled_hess = torch.zeros(2, 2)
sampled_grad_squared = torch.zeros(2, 2)

def log_likelihood_calculator(params):
    dist = Normal(loc=params[0], scale=F.softplus(params[1]))
    obs = dist.sample((num_samples,))
    return dist.log_prob(obs).sum()

for ii in range(num_iters):
    ## Compute gradient (i.e. score) and hessian
    score = jacobian(log_likelihood_calculator, params)
    sampled_grad += score
    sampled_hess += hessian(log_likelihood_calculator, params)
    sampled_grad_squared += torch.outer(score, score)

sampled_grad /= num_iters
sampled_hess /= num_iters
sampled_grad_squared /= num_iters

## Now, compute the FIM by nudging the KL divergence
base_dist = Normal(loc=params[0], scale=F.softplus(params[1]))
def kl_loss(perturbed_params):
    perturbed_dist = Normal(loc=perturbed_params[0], scale=F.softplus(perturbed_params[1]))
    return kl_divergence(p=base_dist, q=perturbed_dist)

## "nudging" params
perturbed_params = params.clone().detach().requires_grad_(True)

kl_hessian = hessian(kl_loss, perturbed_params)

## Compare the results...
print("Averaged Sampled Hessian:")
print(sampled_hess)
print("Averaged Sampled FIM from the score function:")
print(sampled_grad_squared)
print("FIM from nudging the KL divergence:")
print(kl_hessian)
```

This code will show you how the Fisher Information Matrix is related to the hessian of the nudging KL divergence. You can play with the parameters and see how they are related!

</details>